<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-jordan" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Generalized eigenvectors</title>

  <introduction>
    <p>Up to this point, eigenvalues and eigenvectors have helped us
    find some standard forms of operators.  In particular, we have
    seen that if an operator <m>T</m> on <m>V</m>, an
    <m>n</m>-dimensional vector space has <m>n</m> linearly
    independent eigenvectors, then <m>T</m> is diagonalizable.  We
    also know that this condition holds if <m>T</m> has <m>n</m>
    distinct eigenvalues or if the operator is self-adjoint.</p>

    <p>However, there are examples where this does not apply.  For
    instance, the matrix
    <m>A=\begin{bmatrix}
    2 \amp 1 \\
    0 \amp 2 \\
    \end{bmatrix}
    </m>
    has a single eigenvalue <m>\lambda=2</m> and the associated
    eigenspace <m>E_2</m> is one-dimensional.  In this case, the
    characteristic polynomial is <m>(\lambda-2)^2</m> so the
    eigenvalue <m>\lambda=2</m> is a root with multiplicity two.
    </p>

    <p>This example shows that just looking at eigenvalues and
    eigenvectors will not be enough to always find a standard form.
    So instead, we do something that mathematicians love to do:
    generalize an idea that has already proven useful.  In this
    case, the eigenvalue/eigenvector condition is given by the
    equation
    <me>
      (T-\lambda I)\vvec = \zerovec
    </me>.
    We will generalize this equation to
    <me>
      (T-\lambda I)^k\vvec = \zerovec
    </me>
    for some <m>k</m> and call the solutions <em>generalized
    eigenvectors</em>.
    </p>
    

  </introduction>

  <subsection>
    <title> Generalized eigenvectors </title>

    <definition>
      <statement>
	<p>If <m>T</m> is an operator on <m>V</m>, we say that a
	nonzero vector
	<m>\vvec</m> is a <em>generalized eigenvector</em> if
	<m>(T-\lambda I)^k\vvec=\zerovec</m> for some <m>k</m>.  The
	set of such vectors is called the <em>generalized
        eigenspace</em> and is
	denoted <m>G_\lambda</m>.
	</p>
      </statement>
    </definition>

    <p>Notice that eigenvectors are also generalized eigenvectors
    since an eigenvector satisfies <m>(T-\lambda I)\vvec=0</m>,
    the generalized eigenvector condition with <m>k=1</m>.
    </p>

    <p>The next proposition connects generalized eigenspaces
    with the minimal polynomial.</p>

    <proposition>
      <statement>
        <p>If the minimal polynomial of <m>T</m> has the form
        <m>p(x)=q(x)(x-\lambda)^k</m> where <m>q(\lambda)\neq 0</m>,
        then
        <me>
          G_\lambda = \nul((T-\lambda I)^k)
        </me>.
        </p>
      </statement>
      <proof>
        <p>Suppose that <m>\vvec</m> is a generalized eigenvector,
        which means that <m>(T-\lambda I)^l\vvec=0</m> for some
        <m>l</m>.  If <m>s(x) = (x-\lambda)^l</m>, then
        <m>s(T)\vvec=0</m>, which means that <m>p_\vvec</m>, the
        minimal polynomial of <m>\vvec</m>, divides
        <m>s</m>.  Therefore, <m>p_\vvec=(x-\lambda)^m</m> for some
        <m>m\leq l</m>.
        </p>

        <p>The minimal polynomiall <m>p</m> of <m>T</m> has exactly
        <m>k</m> factors of <m>x-\lambda</m>.  
        Since <m>p_\vvec</m> divides <m>p</m>, <m>p_\vvec</m> can have
        no more than <m>k</m> factors of <m>x-\lambda</m>.  Therefore,
        <me>
          (T-\lambda I)^k\vvec = (T-\lambda I)^{k-m}(T-\lambda
          I)^m\vvec = 0
        </me>,
        which says that <m>\vvec</m> is in <m>\nul((T-\lambda
        I)^k)</m>.
        </p>
      </proof>
    </proposition>

    <p>We know that a vector cannot be an eigenvector associated to
    two different eigenvalues.  This is also true for generalized
    eigenvectors.
    </p>

    <proposition xml:id="prop-gen-eig-disjoint">
      <statement>
        <p>If <m>\lambda</m> and <m>\mu</m> are distinct eigenvalues
        of <m>T</m>, then <m>G_\lambda\cap G_\mu=\{0\}</m>.
        </p>
      </statement>
      <proof>
        <p>Suppose that <m>\vvec</m> is a nonzero vector in
        <m>G_\lambda</m>.  Then <m>p_\vvec(x)=(x-\lambda)^m</m> for
        some <m>m</m>, which says that <m>\lambda</m> is a root of
        <m>p_\vvec</m>.
        </p>

        <p>At the same time, if <m>\vvec</m> is in <m>G_\mu</m>, then
        <m>p_\vvec(x) = (x-\mu)^n</m> for some <m>n</m>.  Since
        <me>
          p_\vvec(\lambda) = (\lambda-\mu)^n = 0
        </me>,
        we must have <m>\lambda = \mu</m>.
        </p>
      </proof>
    </proposition>

    <!--
    <p>First notice that <m>G_k</m> is an invariant subspace of
    <m>V</m> because <m>G_k=\nul(p(T))</m> for a polynomial
    <m>p(x)=(x-\lambda)^k</m>.  Moreover, the minimal polynomial of
    <m>T|_{G_k}</m> is <m>p(x)=(x-\lambda)^k</m>.  Since the minimal
    polynomial of <m>T|_{G_k}</m> divides the minimal polynomial of
    <m>T</m>, we know that <m>(x-\lambda)^k</m> divides minimal
    polynomial of <m>T</m>.  This means that <m>\lambda</m> is an
    eigenvalue of <m>T</m>.  So while we have generalized the notion
    of eigenvector, the values of <m>\lambda</m> for which we have
    generalized eigenvectors and still eigenvalues of <m>T</m>.
    </p>

    <proposition>
      <statement>
	<p>A nonzero vector cannot belong to generalized eigenspaces
	corresponding to two different eigenvalues.
	</p>
      </statement>
      <proof>
	<p>Suppose that <m>\lambda</m> and <m>\mu</m> are eigenvalues
	and that <m>(T-\lambda I)^k\vvec=\zerovec</m> for some
	<m>k</m>.  Suppose also that <m>(T-\mu I)^j\vvec=\zerovec</m>
	for some other <m>m</m>.  Then
	<me>
	  \begin{aligned}
	  \zerovec=(T-\mu I)^m\vvec \amp = (T-\lambda I +
	  (\lambda-\mu)I)^m\vvec \\
	  \amp = \sum_{j=0}^m{m \choose
	  j}(\lambda-\mu)^{m-j}(T-\lambda I)^j\vvec \\
	  \end{aligned}
	</me>.
	For this vector <m>\vvec</m>, there is a smallest value of
	<m>k</m> such that <m>(T-\lambda I)^k\vvec=\zerovec</m>, but
	<m>(T-\lambda I)^{k-1}\vvec\neq\zerovec</m>.  We therefore
	multiply both sides of the previous expression by
	<m>(T-\lambda I)^{k-1}</m> so that
	<me>
	  (T-\lambda I)^{k-1}\zerovec =
	  \sum_{j=0}^m(\lambda-\mu)^{m-j} (T-\lambda I)^{k-1+j}\vvec =
	  (\lambda - \mu)^m(T-\lambda I)^{k-1}\vvec
	</me>,
	which says that <m>\lambda - \mu = 0</m>.
	</p>
      </proof>
    </proposition>

    <p>Moreover, a set of nonzero vectors <m>\vvec_j\in
    G_{\lambda_j}</m> for a different set of eigenvalues is linearly
    independent.
    </p>

    <proposition>
      <statement>
        <p>A set of
        generalized eigenvectors, each associated to a different
        eigenvalue, is linearly independent.
        </p>
      </statement>
      <proof>
        <p>Suppose that <m>\vvec_1,\vvec_2,\ldots,\vvec_m</m> is a set
        of generalized eigenvectors associated to different
        eigenvalues.  We will proceed using induction on the number of
        vectors <m>m</m> in the set.  If <m>m=1</m>, we know that
        <me>
          a_1\vvec_1 = \zerovec
        </me>
        implies that <m>a_1=0</m>.
        </p>

        <p>Now suppose that <m>m\gt 1</m> and that the result is true
        for all smaller values of <m>m</m>.
        Suppose that
        <m>(T-\lambda_jI)^{k_j}\vvec_j=\zerovec</m> and that 
        <me>
          a_1\vvec_1+a_2\vvec_2 + \ldots + a_m\vvec_m = \zerovec
        </me>.
        Multiply by <m>(T-\lambda_m)^{k_m}</m> so that the last term
        on the left size becomes zero.  Then we have
        <me>
          a_1(T-\lambda_m)^{k_m}\vvec_1+ \ldots +
          a_m(T-\lambda_m)^{k_m}\vvec_{m-1} = \zerovec
        </me>.

        Since a generalized eigenvector can only be associated to one
        eigenvalue, each vector <m>(T-\lambda_m)^{k_m}\vvec_j</m> is
        nonzero.  In addition, each of these vectors is still a
        generalized eigenvector because
        <me>
          (T-\lambda_j)^{k_j}(T-\lambda_m)^{k_m}\vvec_j =
          (T-\lambda_m)^{k_m}(T-\lambda_j)^{k_j}\vvec_j = \zerovec
        </me>.
        </p>

        <p>Therefore we have a linear combination of <m>m-1</m>
        generalized eigenvectors with
        <me>
          a_1(T-\lambda_m)^{k_m}\vvec_1+ \ldots +
          a_m(T-\lambda_m)^{k_m}\vvec_{m-1} = \zerovec
        </me>.
        By induction <m>a_1=a_2=\ldots=a_{m-1} = 0</m>, which also
        implies that <m>a_m=0</m>.  The original set of <m>m</m>
        generalized eigenvectors is then seen to be linearly
        independent.
        </p>
      </proof>
    </proposition>
    -->
    
  </subsection>

  <subsection>
    <title>Complex Vector Spaces</title>

    <p>Because of the Fundamental Theorem of Algebra, operators on
    complex vector spaces have special properties.  In particular, the
    minimal polynomial of an operator on a complex vector space has
    the form
    <me>
      p(x)=(x-\lambda_1)^{k_1}(x-\lambda_2)^{k_2}\ldots(x-\lambda_m)^{k_m}
    </me>.
    In this case, we claim that <m>V</m> is a direct sum of
    generalized eigenspaces.
    </p>

    <proposition>
      <statement>
        <p>If <m>T</m> is an operator on a complex vector space with
        eigenvalues <m>\basis{\lambda}{m}</m>, then
        <me>
          V = G_{\lambda_1}\oplus G_{\lambda_2} \oplus \ldots
          \oplus G_{\lambda_m}
        </me>.
        </p>
      </statement>

      <proof>
        <p>We will use induction on the dimension of <m>V</m>.  To
        establish the base case, we assume that <m>\dim V = 1</m>.  In
        this case, <m>p(x) = x-\lambda</m> so <m>T-\lambda I = 0</m>
        or <m>T=\lambda I</m>.  Then <m>V=G_\lambda</m>.
        </p>

        <p>For the inductive step, we will assume the result is true
        for all vector spaces of dimension less than
        <m>\dim V</m>.  We will choose an eigenvalue <m>\lambda</m>
        and write the minimal polynomial <m>p</m> as
        <me>
          p(x) = q(x)(x-\lambda)^k
        </me>
        where <m>q(\lambda)\neq 0</m>.
        </p>

        <p>Notice that
        <me>
          \nul((T-\lambda I)^k) \cap \range((T-\lambda)^k) = \{0\}
        </me>.
        To see this, suppose that <m>\vvec</m> is in this
        intersection.  Then <m>\vvec = (T-\lambda)^k\uvec</m> for some
        vector <m>\uvec</m>.  Moreover,
        <me>
          0 = (T-\lambda I)^k\vvec = (T-\lambda I)^{2k}\uvec
        </me>.
        This implies that <m>p_\uvec</m> divides
        <m>(x-\lambda)^{2k}</m> so that the only factors of
        <m>p_\uvec</m> are <m>x-\lambda</m>.  Since <m>p_\uvec</m>
        also divides the minimal polynomial <m>p</m>, we also know
        that <m>p_\uvec</m> divides <m>(x-\lambda)^k</m>.  Therefore,
        <me>
          \vvec = (T-\lambda I)^k\uvec = 0
        </me>.
        </p>

        <p>Because of <xref ref="prop-nul-range-dims"/>, we also know
        that
        <me>
          \dim\nul((T-\lambda I)^k) +\dim \range((T-\lambda I)^k) =
          \dim V
        </me>,
        which says that
        <me>
          V = \nul((T-\lambda I)^k) \oplus \range((T-\lambda I)^k)
        </me>.
        If we define <m>U=\range((T-\lambda I)^k)</m>, then we also
        have
        <me>
          V = G_\lambda \oplus U
        </me>.
        </p>

        <p>Since we have written the minimal polynomial
        <m>p(x)=q(x)(x-\lambda)^k</m>, we can see that the minimal
        polynomial of <m>T|_U</m> is <m>q</m>.  For instance, if
        <m>\uvec</m> is in <m>U</m>, then <m>\uvec=(T-\lambda
        I)^k\vvec</m> for some vector <m>V</m>.  Then
        <me>
          q(T)\uvec = q(T)(T-\lambda I)^k \vvec = p(T)\vvec = 0
        </me>.
        This shows that the minimal polynomial <m>p_U</m> of
        <m>T|_U</m> divides <m>q</m>.  However, if the minimal
        polynomial of <m>T|_U</m> has a smaller degree than <m>q</m>,
        this would contradict the fact that the minimal polynomial of
        <m>T</m> has the smallest possible degree.
        </p>

        <p>By the inductive hypothesis, <m>U</m> may be written as a
        direct sum of its generalized eigenspaces.  All that remains
        is to show that, if <m>\mu</m> is an eigenvalue distinct from
        <m>\lambda</m>, the generalized eigenspace of <m>T</m>
        associated to <m>\mu</m> is the same as the generalized
        eigenspace of <m>T|_U</m> associated to <m>\mu</m>.  To this
        end, suppose that <m>\vvec</m> satisfies <m>(T-\mu
        I)^l\vvec=0</m> for some <m>l</m>.  Because <m>V=G_\lambda
        \oplus U</m>, we can write
        <me>
          \vvec = \nvec + \uvec
        </me>
        where <m>\nvec\in G_\lambda</m> and <m>\uvec \in U</m>.
        We have
        <me>
          0 = (T-\mu I)^l\vvec = (T-\mu I)^l\nvec + (T-\mu I)^l\uvec
        </me>.
        Because these subspaces are <m>T</m>-invariant, we have
        <md>
          <mrow>
            (T-\mu I)^l\nvec \amp = 0
          </mrow>
          <mro>
            (T-\mu I)^l\uvec \amp = 0
          </mro>
        </md>.
        This means that <m>\nvec\in G_\lambda\cap G_\mu = \{0\}</m> by
        <xref ref="prop-gen-eig-disjoint"/> so that <m>\nvec = 0</m>.
        Therefore, <m>\vvec = \uvec\in U</m> and
        <me>
          (T-\mu I)^l\uvec = (T|_U-\mu I)^l \uvec = 0
        </me>.
        </p>
      </proof>
    </proposition>

    <!--
    <proposition>
      <statement>
        <p>If <m>T</m> is an operator on <m>V</m>, a complex vector space,
        then there is a basis of <m>V</m> consisting of generalized
        eigenvectors of <m>T</m>.
        </p>
      </statement>
      <proof>
        <p>Once again we proceed by induction on <m>n</m>, the
        dimension of <m>V</m>.  The result is certainly true if
        <m>n=1</m> because every nonzero vector is an eigenvector of
        <m>T</m>.
        </p>

        <p>Now assume that <m>n\gt 1 </m> and that the result holds
        for all smaller values of <m>n</m>.  Suppose that
        <m>\lambda</m> is an eigenvalue of <m>T</m> and consider the
        operator <m>(T-\lambda I)^n</m>.  We will first explain why
        <me>
          V = \nul(T-\lambda I)^n \oplus \range(T-\lambda I)^n
        </me>.
        </p>

        <p>First, we can see that these subspaces of <m>V</m>
        intersect only in the zero vector.  Suppose that
        <m>\vvec\in\range(T-\lambda I)^n</m>, which means there is a
        vector <m>\uvec</m> such that <m>(T-\lambda I)^n\uvec =
        \vvec</m>.  If <m>\vvec</m> is also in <m>\nul(T-\lambda
        I)^n</m>, then
        <me>
          (T-\lambda I)^n\vvec = (T-\lambda I)^{2n}\uvec = \zerovec
        </me>.
        Because <m>\dim V = n</m>, remember that
        <m>\nul(T-\lambda)^n =\nul(T-\lambda I)^{n+k}</m> for any
        positive <m>k</m>.  Since <m>\uvec\in\nul(T-\lambda
        I)^{2n}</m>, we also have <m>\uvec\in\nul(T-\lambda I)^n</m>,
        which means that
        <me>
          \vvec = (T-\lambda I)^n\uvec = \zerovec
        </me>.
        Therefore, these two subspaces only intersect in the zero
        vector.
        </p>

        <p>In the usual way, the dimensions of these subspaces add to
        <m>\dim V =n</m> so we have
        <me>
          V = \nul(T-\lambda I)^n \oplus \range(T-\lambda I)^n
        </me>.
        If <m>\nul(T-\lambda I)^n = V</m>, then every vector in
        <m>V</m> is a generalized eigenvector of <m>T</m> and the
        conclusion of the theorem holds.  Suppose instead that
        <me>\{\zerovec\} \subsetneq \nul(T-\lambda I)^n \subsetneq V
        </me>
        where the first inclusion holds because <m>\lambda</m> is an
        eigenvalue of <m>T</m>.  This means that
        <me>
          0 \lt \dim \range(T-\lambda I)^n \lt n
        </me>.
        </p>
        
        <p>Now <m>\range(T-\lambda I)^n</m> is invariant under
        <m>T</m>.  By induction, there is a basis of
        <m>\range(T-\lambda I)^n</m> consisting of generalized
        eigenvectors of <m>T</m>.  Putting this together with a basis
        of <m>\nul(T-\lambda I)^n</m> gives a basis of <m>V</m>
        consisting of generalized eigenvectors of <m>T</m>.
        </p>
      </proof>
    </proposition>

    <proposition>
      <statement>
        <p>If <m>T</m> is an operator on the complex vector space
        <m>V</m> and the set of eigenvalues of <m>T</m> is
        <m>\lambda_1,\lambda_2,\ldots,\lambda_m</m>, then
        <me>
          V = G_{\lambda_1}\oplus G_{\lambda_2}\oplus\ldots
          \oplus G_{\lambda_m}
        </me>.
        </p>
      </statement>

      <proof>
        <p> We have seen that there is a basis of <m>V</m> consisting
        of generalized eigenvectors.  Therefore, every vector in
        <m>V</m> can be written as a linear combination of generalized
        eigenvectors.  </p>

        <p>We have further seen a set of generalized eigenvectors
        associated to different eigenvalues is linear independent.
        </p>

        <p>These two facts are enough to establish the theorem.
        </p>
      </proof>
    </proposition>
    -->
  </subsection>

  <subsection>
    <title>Jordan form</title>

    <p>We are now ready to proof the main structure theorem, again
    assuming that <m>V</m> is a complex vector space.  By a Jordan
    block, we mean a matrix <m>J</m> whose diagonal entries are
    all <m>\lambda</m>, whose entries directly above the diagonal are
    1, and whose other entries are all zero.  That is,
    <me>
      J = 
      \begin{bmatrix}
      \lambda \amp 1 \amp 0 \amp \ldots \amp 0 \\
      0 \amp \lambda \amp 1 \amp \ldots \amp 0 \\
      \vdots \amp \vdots \amp \ddots \amp \ldots \amp \vdots \\
      0 \amp 0 \amp 0 \amp \ldots \amp \lambda \\
      \end{bmatrix}
    </me>.
    </p>

    <theorem>
      <title>Jordan canonical form</title>
      <statement>
        <p>If <m>T</m> is an operator on a complex vector space
        <m>V</m>, then there is a basis for <m>V</m> such that the
        matrix associated to <m>T</m> consists only of Jordan blocks
        on the diagonal.  That is,
        <me>
          \begin{bmatrix}
          J_1 \amp 0 \amp \ldots \amp 0 \\
          0 \amp J_2 \amp \ldots \amp 0 \\
          \vdots \amp \vdots \amp \ddots \amp 0 \\
          0 \amp 0 \amp \ldots \amp J_m \\
          \end{bmatrix}
        </me>.
        </p>
      </statement>

      <proof>
        <p>We know that
        <me>
          V = G_{\lambda_1}\oplus G_{\lambda_2}\oplus\ldots
          \oplus G_{\lambda_m}
        </me>.
        Moreover, on <m>G_{\lambda_j}</m>, the operator
        <m>(T-\lambda_j I)^{k_j}</m> is nilpotent, which means there is
        a basis for <m>G_{\lambda_j}</m> such that the matrix
        associated to <m>T-\lambda_j</m> conists of nilpotent
        blocks.  The matrix associated to <m>T</m> therefore
        consists of Jordan blocks.  Because each generalized
        eigenspace is invariant under <m>T</m>, the theorem
        holds. </p>
      </proof>
    </theorem>

    <p>Notice that the characteristic polynomial of <m>T</m>, which
    can easily be found using this matrix, has the form
    <me>
      p(x)=(x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\ldots(x-\lambda_k)^{m_k}
    </me>
    where the multiplicity of each eigenvalue equals the dimension
    <m>\dim G_{\lambda_j}</m>.  In our previous course, we called
    <m>m_j</m> the
    algebraic multiplicity of the eigenvalue <m>\lambda_j</m>.
    Because the eigenspace <m>E_{\lambda_j}\subset G_{\lambda_j}</m>, 
    we have therefore shown that the multiplicity of the eigenvalue is at
    least as large as the dimension of the eigenspace:
    <me>
      0\lt \dim E^{\lambda_j} \leq m_j
    </me>.
    </p>

    <theorem>
      <statement>
        <p>If <m>\lambda</m> is an eigenvalue of the operator <m>T</m>
        whose multiplicity, which is the number of times
        <m>x-\lambda</m> appears as a factor of the characteristic
        polynomial,
        is <m>m</m>, then
        <me>
          \dim E_\lambda \leq m.
        </me>.
        </p>
      </statement>
    </theorem>
  </subsection>
</section>
