<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-lt-intro" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Linear transformations</title>

  <introduction>
    <p>We earlier viewed an <m>m\times n</m> matrix <m>A</m> as
    defining a matrix transformation <m>T:\real^m\to\real^n</m> by
    <m>T(\vvec)=A\vvec</m>.  Due to the linearity of matrix
    multiplication, this meant that
    <md>
      <mrow>T(s\vvec)\amp=sT(\vvec)</mrow>
      <mrow>T(\vvec_1+\vvec_2)\amp =T(\vvec_1)+T(\vvec_2)</mrow>
    </md>.
    Now that we are working with vector spaces, we can define linear
    transformations, which are functions between vector spaces that
    satisfy these properties.
    </p>
  </introduction>

  <subsection>
    <title> Linear transformations </title>

    <p>Given two vector spaces, <m>V</m> and <m>W</m>, we can define a
    linear transformation between them by generalizing our earlier
    notion of matrix transformation.</p>

    <definition>
      <statement>
        <p>If <m>V</m> and <m>W</m> are vector spaces, then a
        <em>linear transformation</em> is a function <m>T:V\to W</m>
        such that, for every scalar <m>s</m> and pair of vectors
        <m>\vvec_1,\vvec_2\in V</m>, we have
        <md>
          <mrow>T(s\vvec)\amp=sT(\vvec)</mrow>
          <mrow>T(\vvec_1+\vvec_2)\amp =T(\vvec_1)+T(\vvec_2)</mrow>
        </md>.
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>Suppose that
        <m>A=\begin{bmatrix}
        2 \amp 0 \\
        -1 \amp 2 \\
        2 \amp 1 \\
        \end{bmatrix}
        </m>.  Then <m>T:\real^2\to\real^3</m> defined by
        <m>T(\vvec)=A\vvec</m> is a linear transformation.
        </p>
        <p>This follows because matrix multiplication is a linear
        operation:
        <md>
          <mrow>
            T(s\vvec) \amp = A(s\vvec) = sA\vvec = sT(\vvec)
          </mrow>
          <mrow>
            T(\vvec_1+\vvec_2) \amp = A(\vvec_1+\vvec_2) = A\vvec_1 +
            A\vvec_2 = T(\vvec_1) +T(\vvec_2)
          </mrow>
        </md>
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\fcal</m>, the set of functions
        <m>f:\real\to\complex</m>.  Then
        <m>T(f)=\threevec{f(-3)}{f(0)}{f(3)}</m> is a linear 
        transformation <m>T:\fcal\to \complex^3</m>.
        </p>
        <p>To see this, we need to remember how scalar multiplication
        and vector addition work in <m>V</m>.  If <m>s</m> is a scalar
        and <m>f_1</m> and <m>f_2</m> are functions, then
        <md>
          <mrow>
            (sf)(x) \amp = s(f(x))
          </mrow>
          <mrow>
            (f_1+f_2)(x) \amp = f_1(x) + f_2(x)
          </mrow>
        </md>
        Therefore,
        <md>
          <mrow>
            T(sf) \amp = \threevec{(sf)(-3)}{(sf)(0)}{(sf)(3)}
            = \threevec{sf(-3)}{sf(0)}{sf(3)}
            = s\threevec{f(-3)}{f(0)}{f(3)} = sT(f)
          </mrow>
          <mrow>
            T(f_1+f_2) \amp = \threevec{(f_1+f_2)(-3)}
            {(f_1+f_2)(0)}{(f_1+f_2)(3)}
            = \threevec{f_1(-3)+f_2(-3)}{f_1(0)+f_2(0)}{f_1(3)+f_2(3)}
          </mrow>
          <mrow>\amp
            =
            \threevec{f_1(-3)}{f_1(0)}{f_1(3)}+\threevec{f_2(-3)}{f_2(0)}
            {f_2(3)}=T(f_1)+T(f_2)
          </mrow>
        </md>
        </p>
        
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\pbb_3</m> and <m>W=\pbb_2</m>.  If
        <m>p</m> is a polynomial in <m>\pbb_3</m>, define the function
        <m>T(p) = p'</m> where <m>p'</m> is the derivative of
        <m>p</m>.  Two common rules of differentiation, the constant
        multiplier rule and the addition rule, imply that <m>T</m> is
        a linear transformation.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>If <m>V=\pbb_3</m> and <m>W=\real</m>, then
        <m>T(p)=\int_0^5 p(x)~dx</m> is a linear transformation.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V</m> is the vector space of <m>3\times3</m>
        matrices.  Then <m>T:V\to\real</m> by <m>T(A)=\det(A)</m>
        is not a linear 
        transformation because <m>T(sA)=s^3T(A)</m>.
        </p>
      </statement>
    </example>

    <definition>
      <statement>
        <p>Given two vector spaces, <m>V</m> and
        <m>W</m>, the set of 
        linear transformations <m>T:V\to W</m> will be denoted as
        <m>L(V,W)</m>.</p>
      </statement>
    </definition>

    <!--

    <proposition>
      <statement>
        <p>Given two vector spaces <m>V</m> and <m>W</m> over
        <m>\field</m>, the set
        <m>L(V,W)</m> is a vector space over <m>\field</m> having
        <me>
          \dim L(V,W) = \dim(V)\dim(W)
        </me>.
        </p>
      </statement>

      <proof>
        <p>First, notice that we may define scalar multiplication and
        vector addition on
        <m>L(V,W)</m>.  In particular, if <m>T</m> and <m>S</m> are in
        <m>L(V,W)</m>, then
        <md>
          <mrow>
            (sT)(\vvec) \amp = s(T(\vvec))
          </mrow>
          <mrow>
            (T+S)(\vvec) \amp = T(\vvec)+S(\vvec)
          </mrow>
        </md>.
        With these operations, <m>L(V,W)</m> becomes a vector space.
        </p>
        
        <p>Let <m>\basis{\vvec}{n}</m> be a basis for <m>V</m> and
        <m>\basis{\wvec}{m}</m> be a basis for <m>W</m>.  If <m>T:V\to
        W</m> is a linear transformation, then there is an <m>m\times
        n</m> matrix <m>A</m> such that
        <me>
          T(\vvec_j) = A_{1,j}\wvec_1 + A_{2,j}\wvec_2 + \ldots +
          A_{m,j}\wvec_m
        </me>.
        We choose <m>j</m> and <m>k</m> and define the linear
        transformation <m>T_{k,j}</m> by
        <me>
          T_{k,j}(\vvec_i)= \begin{cases}
          \wvec_k \amp \text{if } \ i=j \\
          0 \amp \text{otherwise}
          \end{cases}
        </me>.
        Then every linear transformation is a linear combination of
        the <m>T_{j,k}</m> and this set is linearly independent.
        </p>
      </proof>
    </proposition>
    -->

    <assemblage>
      <title>Notation</title>
        <p>While a linear transformation <m>T:V\to W</m> is a
        function, we will frequently write <m>T\vvec</m>, without
        parentheses, when we mean <m>T(\vvec)</m>.  This is similar to
        how we often write <m>\sin x</m> rather than <m>\sin(x)</m> in
        other courses.
        </p>
    </assemblage>
      
  </subsection>

  <subsection>
    <title>The null space and range</title>

    <p>A linear transformation <m>T:V\to W</m> creates a subspace of
    <m>V</m> and a subspace of <m>W</m>.</p>

    <definition>
      <statement>
        <p>If <m>T:V\to W</m> is a linear transformation, we define
        the <em>null space</em> and <em>range</em> of <m>T</m> to be
        <md>
          <mrow>\nul(T)\amp = \{\vvec~|~T\vvec=0\}\subset V</mrow>
          <mrow>\range(T)\amp = \{\wvec~|~\wvec=T\vvec \text{ for some
          }\vvec\}\subset W</mrow>
        </md>.
        </p>
      </statement>
    </definition>

    <p>In our earlier linear algebra courses, we considered the null
    space and column space of a matrix.  The null space and range of a
    linear transformation is the same concept generalized to vector
    spaces.
    </p>

    <example>
      <statement>
        <p>Suppose that <m>A</m> is a <m>10\times 17</m> matrix and
        consider the 
        linear transformation <m>T:\real^{17}\to\real^{10}</m>.  Then
        <m>\nul(A)</m> is the set of solutions to the equation
        <m>A\xvec=T(\xvec)=\zerovec</m>, which is the same as the null
        space <m>\nul(T)</m>.
        </p>
        <p>Similarly, the column space is the set of vectors
        <m>\bvec</m> for which <m>A\xvec = \bvec</m> is consistent.
        In other words, <m>\bvec</m> is in <m>\col(A)</m> if and only
        if there is a vector <m>\xvec</m> such that <m>T(\xvec)=A\xvec
        = \bvec</m>.  This is precisely the definition of
        <m>\range(T)</m>.
        </p>
      </statement>
    </example>
    
    <example>
      <statement>
        <p>Consider the linear transformation
        <m>T:\real^4\to\real^3</m> defined by the matrix
        <me>
          A=\begin{bmatrix}
          2 \amp -1 \amp 2 \amp 3 \\
          1 \amp 0 \amp 0 \amp 2 \\
          -2 \amp 2 \amp -4 \amp -2 \\
          \end{bmatrix}
          \sim
          \begin{bmatrix}
          1 \amp 0 \amp 0 \amp 2 \\
          0 \amp 1 \amp -2 \amp 1 \\
          0 \amp 0 \amp 0 \amp 0 \\
          \end{bmatrix}
        </me>.
        The null space is the set of vectors for which
        <m>T\vvec=A\vvec=\zerovec</m>, which we see
        is the subspace of <m>\real^4</m> spanned by
        <m>\fourvec0210</m> and <m>\fourvec{-2}{-1}01</m>.
        </p>

        <p>Similarly, <m>\range(T)</m> is the subspace of
        <m>\real^3</m> having a basis 
        given by <m>\threevec21{-2}</m> and
        <m>\threevec{-1}02</m>. 
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\pbb_3</m> and <m>W=\real^2</m> and that
        <m>T:V\to\real^2</m> where <m>T(p)=\twovec{p(0)}{p'(0)}</m>.
        A general polynomial in <m>V</m> has the form
        <me>
          p(x)=a_3x^3+a_2x^2+a_1x+a_0
        </me>
        so that <m>T(p)=\twovec{a_0}{a_1}</m>.  Therefore,
        <m>\nul(T)</m> is the set of polynomials for which
        <m>a_0=a_0=0</m> so that <m>p(x)=a_3x^3+a_2x^2</m>.  We also
        see that
        <m>\range(T) = \real^2</m>
        </p>
      </statement>
    </example>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m>, then <m>\nul(T)</m> is a subspace of
        <m>V</m> and <m>\range(T)</m> is a subspace of <m>W</m>.
        </p>
      </statement>

      <proof>
        <p>Suppose that <m>\vvec_1</m> and <m>\vvec_2</m> are in
        <m>\nul(T)</m>.  Then we have
        <md>
          <mrow>
            T(s\vvec_1) = \amp s(T\vvec_1) = s0 = 0
          </mrow>
          <mrow>
            T(\vvec_1+\vvec_2) = \amp T(\vvec_1) + T(\vvec_2) = 0 + 0
            = 0.
          </mrow>
        </md>
        This shows that <m>\nul(T)</m> is closed under scalar
        multiplication and vector addition and is therefore a subspace
        of <m>V</m>.
        </p>
        <p>Now suppose the <m>\wvec_1</m> and <m>\wvec_2</m> are in
        <m>\range(T)</m>.  We know that there are vectors
        <m>\vvec_1</m> and <m>\vvec_2</m> in <m>V</m> such that
        <m>T(\vvec_1) = \wvec_1</m> and <m>T(\vvec_2)=\wvec_2</m>.
        Therefore,
        <md>
          <mrow>
            s\wvec_1\amp = sT(\vvec_1) = T(s\vvec_1)
          </mrow>
          <mrow>
            \wvec_1+\wvec_2 \amp = T(\vvec_1)+T(\vvec_2) =
            T(\vvec_1+\vvec_2).
          </mrow>
        </md>
        This shows that <m>\range(T)</m> is closed under scalar
        multiplication and vector addition so <m>\range(T)</m> is a
        subspace of <m>W</m>.
        </p>
      </proof>
    </proposition>
    
  </subsection>

  <subsection>
    <title>Vector space isomorphisms</title>

    <definition>
      <statement>
        <p>Suppose <m>T:V\to W</m>. If <m>\range(T)=W</m>, we say that
        <m>T</m> is <em>surjective</em>.  If <m>\nul(T)=\{0\}</m>, we
        say that 
        <m>T</m> is <em>injective</em>.
        </p>
      </statement>
    </definition>

    <p>If <m>T</m> is surjective, notice that every vector <m>\wvec\in
    W</m> has a vector <m>\vvec</m> for which <m>T\vvec=\wvec</m>.
    </p>

    <p>If <m>T</m> is injective and <m>T\vvec_1=T\vvec_2</m>, then
    <m>\vvec_1=\vvec_2</m> since <m>T(\vvec_1-\vvec_2)=0</m> meaning
    <m>\vvec_1-\vvec_2\in \nul(T)</m>.
    </p>

    <example>
      <statement>
        <p>Once again, these are familiar notions.  Suppose that
        <m>A</m> is an <m>m\times n</m> matrix that defines a 
        linear transformation <m>T:\real^n\to\real^m</m>.  Then
        <m>T</m> is injective if <m>\nul(A) = \{\zerovec\}</m>.  This
        happens when the columns of <m>A</m> are linearly independent.
        </p>
        <p> The transformation
        <m>T</m> is surjective if <m>\col(A) = \real^m</m>, which
        happens when the columns of <m>A</m> span <m>\real^m</m>.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>The linear transformation <m>T:\pbb\to\pbb</m> defined by
        <m>T(p)(x)=xp(x)</m> is injective but not surjective because
        the constant polynomials are not in <m>\range(T)</m>.
        </p>
      </statement>
    </example>
  
    

    <!--

    <p>We will frequently make use of the next proposition.</p>
    <proposition xml:id="prop-nul-range-dims">
      <statement>
        <p>If <m>T:V\to W</m> is a linear transformation, then
        <m>\nul(T)</m> is a subspace of <m>V</m> and
        <m>\range(T)</m> is a subspace of <m>W</m>.  Moreover,
        <me>
          \dim \nul(T) + \dim \range(T) = \dim V
          </me>.
        </p>
      </statement>
      <proof>
        <p>First, suppose that <m>\vvec_1</m> and <m>\vvec_2</m> are
        in <m>\nul(T)</m>.  Then
        <md>
          <mrow>T(s\vvec_1) \amp = sT\vvec_1 = 0</mrow>
          <mrow>T(\vvec_1+\vvec_2) \amp = T\vvec_1 +T\vvec_2=
          0</mrow>
          </md>,
          which shows that <m>\nul(T)</m> is closed under scalar
          multiplication and addition and is therefore a subspace.
        </p>
        
        <p>In the same way, suppose that <m>\wvec_1</m> and
        <m>\wvec_2</m> are in <m>\range(T)</m>.  We must have vectors
        <m>\vvec_1</m> and <m>\vvec_2</m> such that
        <m>T\vvec_1=\wvec_1</m> and <m>T\vvec_2=\wvec_2</m>.  It
        then follows that
        <md>
          <mrow>T(s\vvec_1) \amp = sT\vvec_1 = s\wvec</mrow>
          <mrow>T(\vvec_1+\vvec_2) \amp = T\vvec_1 + T\vvec_2=
          \wvec_1+\wvec_2</mrow>
          </md>,
          which shows that <m>\range(T)</m> is a subspace of <m>W</m>.
        </p>
        
        <p>Now suppose that <m>\uvec_1,\ldots,\uvec_j</m> is a
        basis for <m>\nul(T)</m>, which we extend to a basis for
        <m>V</m> by adding vectors <m>\vvec_1,\ldots,\vvec_k</m>.
        We also define <m>\wvec_i=T\vvec_i</m>.</p>
        
        <p>Given a vector <m>\vvec</m> in <m>V</m>, we can write
        <me>
          \vvec=a_1\uvec_1 + \ldots + a_j\uvec_j + b_1\vvec_1 +
          \ldots + b_k\vvec_k
        </me>
        so that
        <md>
          <mrow>
            T\vvec\amp=a_1T\uvec_1 + \ldots + a_jT\uvec_j + b_1T\vvec_1 +
            \ldots + b_k\vvec_k
          </mrow>
          <mrow>
            \amp=b_1\wvec_1+\ldots+b_k\wvec_k
          </mrow>
          </md>.
          This shows that <m>\wvec_1,\ldots,\wvec_k</m> span
          <m>\range(T)</m>.
        </p>
        
        <p>We also claim that <m>\wvec_1,\ldots,\wvec_k</m> forms a
        linearly independent set.  Suppose that
        <md>
          <mrow>
            b_1\wvec_1 + \ldots b_k\wvec_k \amp = 0
          </mrow>
          <mrow>
            T(b_1\vvec_1+\ldots+b_k\vvec_k) \amp = 0
          </mrow>
          </md>,
          which means that <m>b_1\vvec_1+\ldots+b_k\vvec_k</m> is in
          <m>\nul(T)</m> so that this vector is a linear combination
          of <m>\uvec_1,\ldots,\uvec_j</m>.  This can only happen if
          the vector is zero, showing that
          <m>b_1=\ldots=b_k=0</m> and the <m>\wvec</m> vectors are
        linearly independent.</p>
        
        <p>We conclude that <m>\wvec_1,\ldots,\wvec_k</m> is a basis
        for <m>\range(T)</m> and we have
        <me>
          \dim \nul(T) + \dim \range(T) = j + k = \dim V
          </me>.
        </p>
      </proof>
    </proposition>

    <p>The following proposition follows immediately from <xref
    ref="prop-nul-range-dims"/>. 
    </p>
    
    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> is surjective, then <m>\dim V \geq \dim
        W</m>.
        </p>

        <p>Similarly, if <m>T:V\to W</m> is injective, then <m>\dim V
        \leq \dim W</m>.
        </p>
      </statement>
    </proposition>
    -->

    <definition>
      <statement>
        <p>A linear transformation <m>T:V\to W</m> is called an
        <em>isomorphism</em> if <m>T</m> is both surjective and
        injective.
        </p>
      </statement>
    </definition>

    <!--

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> is an isomorphism, then <m>\dim V = \dim
        W</m>.  Moreover, for every <m>\wvec\in W</m>, there is a
        unique <m>\vvec</m> such that <m>\wvec=T\vvec</m>.
        </p>
      </statement>
    </proposition>

    -->

    <example>
      <statement>
        <p>Consider the linear transformation <m>T:\pbb_2\to\real^3</m>
        defined by <m>T(p)=\threevec{p(0)}{p'(0)}{p''(0)}</m>.
        If <m>p(x)=a_0+a_1x+a_2x^2</m>, then
        <m>T(p) = \threevec{a_0}{a_1}{2a_2}</m>.  This shows that
        <m>\nul(T) = \{0\}</m> and <m>\range(T)=\real^3</m>.
        Therefore, <m>T</m> is a vector space isomorphism.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>If <m>V</m> is a vector space, then <m>I:V\to V</m> defined
        by <m>I(\vvec) = \vvec</m> is a
        linear transformation called the <em>identity</em>
        transformation.
        </p>
      </statement>
    </example>

    <p>Suppose that <m>T:V\to W</m> is an isomorphism, then every
    vector <m>\wvec</m> in <m>W</m> has a vector <m>\vvec</m> in
    <m>V</m> such that <m>T(\vvec) = \wvec</m>.  In fact, there is
    exactly once such vector since if <m>T(\vvec_1)=T(\vvec_2) =
    \wvec</m>, we know that <m>\vvec_1=\vvec_2</m> because <m>T</m> is
    injective.  In this case, we can define a function <m>S:W\to V</m>
    where <m>S(\wvec)</m> is the vector <m>\vvec</m> for which
    <m>T(\vvec) = \wvec</m>.
    </p>

    <p>Notice that <m>S:W\to V</m> is a linear transformation.  For
    instance, if <m>T(\vvec) = \wvec</m>, then <m>T(s\vvec) =
    s\wvec</m>, which says that
    <me>
      S(s\wvec) = s\vvec = sS(\wvec)
    </me>.
    In the same way, we have
    <me>
      S(\wvec_1+\wvec_2) = \vvec_1+\vvec_2 = S(\wvec_1)+S(\wvec_2)
    </me>.
    </p>

    <p>Therefore, we have the following proposition.
    </p>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> is an ismorphism, there is a linear
        transformation <m>S:W\to V</m> such that <m>ST=I_V</m>, the
        identity transformation on <m>V</m>, and <m>TS=I_W</m>, the
        identity transformation on <m>W</m>.  We will typically denote
        <m>S</m> as <m>T^{-1}</m>.
        </p>
      </statement>
    </proposition>

    <proposition xml:id="prop-fin-dim-isom">
      <statement>
        <p>If <m>V</m> is a finite dimensional vector space of
        dimension <m>n</m> over the
        field <m>\field</m>, then there is an isomorphism
        <m>T:\field^n\to V</m>.
        </p>
      </statement>

      <proof>
        <p>We choose a basis <m>\basis{\vvec}{n}</m> and define
        <me>
          T\left(\threevec{c_1}{\vdots}{c_n}\right) = c_1\vvec_1 +
          c_2\vvec_2 + \ldots + c_n\vvec_n
        </me>.
        By the linear independence of the basis, we see that <m>T</m>
        is injective.  Since the span of the basis vectors is
        <m>V</m>, we see that <m>T</m> is surjective.
        </p>
      </proof>
    </proposition>

    <p>The term <em>isomorphism</em> means <q>having the same shape or
    structure.</q>  In other words, isomorphic vector spaces have the
    same structure.  In our earlier courses, we considered only the
    vector spaces <m>\real^n</m>.  The previous proposition, <xref
    ref="prop-fin-dim-isom"/>, shows us that every finite dimensional
    real vector space has the same structure as <m>\real^n</m>.
    This means that, technically speaking, we were also studying
    finite dimensional real vector spaces at the same time.
    </p>

  </subsection>
  <!--
  <subsection>
    <title>Representing linear transformations with matrices</title>

    <p>We began this section by recalling our earlier study of matrix
    transformations and used that as motivation for introducing linear
    transformations.  However, the connection is closer than just that.
    Suppose that <m>T:V\to W</m> is a linear transformation and that
    we have a basis <m>\bcal=\{\vvec_1,\ldots,\vvec_n\}</m> for
    <m>V</m> and a basis <m>\ccal=\{\wvec_1,\ldots,\wvec_m\}</m> for
    <m>W</m>.  We then have
    <me>
      T\vvec_j = A_{1,j}\wvec_1 + A_{2,j}\wvec_2 + \ldots + A_{m,j}\wvec_m
    </me>,
    which defines an <m>m\times n</m> matrix <m>A</m>.
    </p>

    <definition>
      <statement>
        <p>If <m>T:V\to W</m> is a linear transformation, <m>\bcal</m>
        a basis for <m>V</m> and <m>\ccal</m> a basis for <m>W</m>, we
        say that this matrix <m>M_T^{\ccal,\bcal}</m> is the matrix
        associated to <m>T</m> with respect to these bases.  We will
        frequently drop the subscript when the transformation is clear
        from context.</p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>Consider <m>T:\pbb_3\to\pbb_2</m> where <m>T(p)=p'</m>.  If
        we choose the bases <m>\bcal=\{x^3,x^2,x,1\}</m> and
        <m>\ccal=\{x^2,x,1\}</m>, then
        <me>
          M^{\bcal,\ccal}=\begin{bmatrix}
          3 \amp 0 \amp 0 \amp 0 \\
          0 \amp 2 \amp 0 \amp 0 \\
          0 \amp 0 \amp 1 \amp 0 \\
          \end{bmatrix}
        </me>.
        </p>
      </statement>
    </example>

    <proposition>
      <statement>
        <p>If <m>\bcal</m> and <m>\ccal</m> are
        bases for <m>V</m> and <m>W</m> respectively, then
        the map that assigns <m>T\to M_T^{\ccal,\bcal}</m> is a linear
        transformation.  More specifically,
        <md>
          <mrow>
            M_{cT}^{\ccal,\bcal} \amp = cM_T^{\ccal,\bcal}
          </mrow>
          <mrow>
            M_{T+S}^{\ccal,\bcal} \amp = M_T^{\ccal,\bcal} +
            M_S^{\ccal,\bcal}.
          </mrow>
        </md>
        </p>
      </statement>
    </proposition>

    <p> The next proposition says that the composition of linear
    transformations corresponds to matrix multiplication.</p>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> and <m>S:W\to U</m> are linear
        transformations and <m>\bcal</m>, <m>\ccal</m>, and
        <m>\dcal</m> are bases for <m>V</m>, <m>W</m>, and <m>U</m>,
        respectively, then
        <me>
          M_{ST}^{\dcal, \bcal} = M_S^{\dcal,\ccal}M_T^{\ccal,\bcal}
        </me>.
        </p>
      </statement>

      <proof>
        <p>We denote the vectors in the bases by
        <m>\basis{\vvec}{m}</m>, <m>\basis{\wvec}{n}</m>, and
        <m>\basis{\uvec}{p}</m>, respectively.  Similarly, we use the
        shorthand 
        <sidebyside widths="10% 10% 10%" margins="5% 5%">
          <p><m> A = M_T^{\ccal,\bcal}</m></p>
          <p><m> B = M_S^{\dcal,\ccal}</m></p>
          <p><m> C = M_{ST}^{\dcal,\bcal}</m></p>
        </sidebyside>
        </p>
        <p> We have
        <md>
          <mrow>
            T\vvec_j \amp = \sum_k A_{k,j} \wvec_k
          </mrow>
          <mrow>
            S\wvec_k \amp = \sum_l B_{l,k} \uvec_l
          </mrow>
        </md>,
        which implies that
        <md>
          <mrow>
            (ST)\vvec_j \amp = S\left(\sum_kA_{k,j}\wvec_k\right)
          </mrow>
          <mrow>
            \amp = \sum_k A_{k,j} S(\wvec_k)
          </mrow>
          <mrow>
            \amp = \sum_k A_{k,j} \sum_l B_{l,k}\uvec_l
          </mrow>
          <mrow>
            \amp = \sum_l \sum_k B_{l,k}A_{k,j} \uvec_l
          </mrow>
          <mrow>
            \amp = \sum_l C_{l,j} \uvec_l.
          </mrow>
        </md>
        Therefore, <m>C_{l,j} = \sum_kB_{l,k}A_{k,j}</m>, which says
        that <m>C=BA</m> as expected.
        </p>
      </proof>
          
    </proposition>
  </subsection>

  

      
  <subsection>
    <title>Dual spaces</title>

  </subsection>
  -->
      
</section>
