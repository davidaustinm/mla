<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-lt-intro" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Linear transformations</title>

  <introduction>
    <p>We earlier viewed an <m>m\times n</m> matrix <m>A</m> as
    defining a matrix transformation <m>T:\real^m\to\real^n</m> by
    <m>T(\vvec)=A\vvec</m>.  Due to the linearity of matrix
    multiplication, this meant that
    <md>
      <mrow>T(s\vvec)\amp=sT(\vvec)</mrow>
      <mrow>T(\vvec_1+\vvec_2)\amp =T(\vvec_1)+T(\vvec_2)</mrow>
    </md>.
    Now that we are working with vector spaces, we can define linear
    transformations, which are functions between vector spaces that
    satisfy these properties.
    </p>
  </introduction>

  <subsection>
    <title> Linear transformations </title>

    <p>Given two vector spaces, <m>V</m> and <m>W</m>, we can define a
    linear transformation between them by generalizing our earlier
    notion of matrix transformation.</p>

    <definition>
      <statement>
        <p>If <m>V</m> and <m>W</m> are vector spaces, then a
        <em>linear transformation</em> is a function <m>T:V\to W</m>
        such that, for every scalar <m>s</m> and pair of vectors
        <m>\vvec_1,\vvec_2\in V</m>, we have
        <md>
          <mrow>T(s\vvec)\amp=sT(\vvec)</mrow>
          <mrow>T(\vvec_1+\vvec_2)\amp =T(\vvec_1)+T(\vvec_2)</mrow>
        </md>.
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>Suppose that
        <m>A=\begin{bmatrix}
        2 \amp 0 \\
        -1 \amp 2 \\
        2 \amp 1 \\
        \end{bmatrix}
        </m>.  Then <m>T:\real^2\to\real^3</m> defined by
        <m>T(\vvec)=A\vvec</m> is a linear transformation.
        </p>
        <p>This follows because matrix multiplication is a linear
        operation:
        <md>
          <mrow>
            T(s\vvec) \amp = A(s\vvec) = sA\vvec = sT(\vvec)
          </mrow>
          <mrow>
            T(\vvec_1+\vvec_2) \amp = A(\vvec_1+\vvec_2) = A\vvec_1 +
            A\vvec_2 = T(\vvec_1) +T(\vvec_2)
          </mrow>
        </md>
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\fcal</m>, the set of functions
        <m>f:\real\to\complex</m>.  Then
        <m>T(f)=\threevec{f(-3)}{f(0)}{f(3)}</m> is a linear 
        transformation <m>T:\fcal\to \complex^3</m>.
        </p>
        <p>To see this, we need to remember how scalar multiplication
        and vector addition work in <m>V</m>.  If <m>s</m> is a scalar
        and <m>f_1</m> and <m>f_2</m> are functions, then
        <md>
          <mrow>
            (sf)(x) \amp = s(f(x))
          </mrow>
          <mrow>
            (f_1+f_2)(x) \amp = f_1(x) + f_2(x)
          </mrow>
        </md>
        Therefore,
        <md>
          <mrow>
            T(sf) \amp = \threevec{(sf)(-3)}{(sf)(0)}{(sf)(3)}
            = \threevec{sf(-3)}{sf(0)}{sf(3)}
            = s\threevec{f(-3)}{f(0)}{f(3)} = sT(f)
          </mrow>
          <mrow>
            T(f_1+f_2) \amp = \threevec{(f_1+f_2)(-3)}
            {(f_1+f_2)(0)}{(f_1+f_2)(3)}
            = \threevec{f_1(-3)+f_2(-3)}{f_1(0)+f_2(0)}{f_1(3)+f_2(3)}
          </mrow>
          <mrow>\amp
            =
            \threevec{f_1(-3)}{f_1(0)}{f_1(3)}+\threevec{f_2(-3)}{f_2(0)}
            {f_2(3)}=T(f_1)+T(f_2)
          </mrow>
        </md>
        </p>
        
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\pbb_3</m> and <m>W=\pbb_2</m>.  If
        <m>p</m> is a polynomial in <m>\pbb_3</m>, define the function
        <m>T(p) = p'</m> where <m>p'</m> is the derivative of
        <m>p</m>.  Two common rules of differentiation, the constant
        multiplier rule and the addition rule, imply that <m>T</m> is
        a linear transformation.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>If <m>V=\pbb_3</m> and <m>W=\real</m>, then
        <m>T(p)=\int_0^5 p(x)~dx</m> is a linear transformation.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V</m> is the vector space of <m>3\times3</m>
        matrices.  Then <m>T:V\to\real</m> by <m>T(A)=\det(A)</m>
        is not a linear 
        transformation because <m>T(sA)=s^3T(A)</m>.
        </p>
      </statement>
    </example>

    <definition xml:id="definition-l-v-w">
      <statement>
        <p>Given two vector spaces, <m>V</m> and
        <m>W</m>, the set of 
        linear transformations <m>T:V\to W</m> will be denoted as
        <m>L(V,W)</m>.</p>
      </statement>
    </definition>

    <!--

    <proposition>
      <statement>
        <p>Given two vector spaces <m>V</m> and <m>W</m> over
        <m>\field</m>, the set
        <m>L(V,W)</m> is a vector space over <m>\field</m> having
        <me>
          \dim L(V,W) = \dim(V)\dim(W)
        </me>.
        </p>
      </statement>

      <proof>
        <p>First, notice that we may define scalar multiplication and
        vector addition on
        <m>L(V,W)</m>.  In particular, if <m>T</m> and <m>S</m> are in
        <m>L(V,W)</m>, then
        <md>
          <mrow>
            (sT)(\vvec) \amp = s(T(\vvec))
          </mrow>
          <mrow>
            (T+S)(\vvec) \amp = T(\vvec)+S(\vvec)
          </mrow>
        </md>.
        With these operations, <m>L(V,W)</m> becomes a vector space.
        </p>
        
        <p>Let <m>\basis{\vvec}{n}</m> be a basis for <m>V</m> and
        <m>\basis{\wvec}{m}</m> be a basis for <m>W</m>.  If <m>T:V\to
        W</m> is a linear transformation, then there is an <m>m\times
        n</m> matrix <m>A</m> such that
        <me>
          T(\vvec_j) = A_{1,j}\wvec_1 + A_{2,j}\wvec_2 + \ldots +
          A_{m,j}\wvec_m
        </me>.
        We choose <m>j</m> and <m>k</m> and define the linear
        transformation <m>T_{k,j}</m> by
        <me>
          T_{k,j}(\vvec_i)= \begin{cases}
          \wvec_k \amp \text{if } \ i=j \\
          0 \amp \text{otherwise}
          \end{cases}
        </me>.
        Then every linear transformation is a linear combination of
        the <m>T_{j,k}</m> and this set is linearly independent.
        </p>
      </proof>
    </proposition>
    -->

    <assemblage>
      <title>Notation</title>
        <p>While a linear transformation <m>T:V\to W</m> is a
        function, we will frequently write <m>T\vvec</m>, without
        parentheses, when we mean <m>T(\vvec)</m>.  This is similar to
        how we often write <m>\sin x</m> rather than <m>\sin(x)</m> in
        other courses.
        </p>
    </assemblage>
      
  </subsection>

  <subsection>
    <title>The null space and range</title>

    <p>A linear transformation <m>T:V\to W</m> creates a subspace of
    <m>V</m> and a subspace of <m>W</m>.</p>

    <definition>
      <statement>
        <p>If <m>T:V\to W</m> is a linear transformation, we define
        the <em>null space</em> and <em>range</em> of <m>T</m> to be
        <md>
          <mrow>\nul(T)\amp = \{\vvec~|~T\vvec=0\}\subset V</mrow>
          <mrow>\range(T)\amp = \{\wvec~|~\wvec=T\vvec \text{ for some
          }\vvec\}\subset W</mrow>
        </md>.
        </p>
      </statement>
    </definition>

    <p>In our earlier linear algebra courses, we considered the null
    space and column space of a matrix.  The null space and range of a
    linear transformation is the same concept generalized to vector
    spaces.
    </p>

    <example>
      <statement>
        <p>Suppose that <m>A</m> is a <m>10\times 17</m> matrix and
        consider the 
        linear transformation <m>T:\real^{17}\to\real^{10}</m>.  Then
        <m>\nul(A)</m> is the set of solutions to the equation
        <m>A\xvec=T(\xvec)=\zerovec</m>, which is the same as the null
        space <m>\nul(T)</m>.
        </p>
        <p>Similarly, the column space is the set of vectors
        <m>\bvec</m> for which <m>A\xvec = \bvec</m> is consistent.
        In other words, <m>\bvec</m> is in <m>\col(A)</m> if and only
        if there is a vector <m>\xvec</m> such that <m>T(\xvec)=A\xvec
        = \bvec</m>.  This is precisely the definition of
        <m>\range(T)</m>.
        </p>
      </statement>
    </example>
    
    <example>
      <statement>
        <p>Consider the linear transformation
        <m>T:\real^4\to\real^3</m> defined by the matrix
        <me>
          A=\begin{bmatrix}
          2 \amp -1 \amp 2 \amp 3 \\
          1 \amp 0 \amp 0 \amp 2 \\
          -2 \amp 2 \amp -4 \amp -2 \\
          \end{bmatrix}
          \sim
          \begin{bmatrix}
          1 \amp 0 \amp 0 \amp 2 \\
          0 \amp 1 \amp -2 \amp 1 \\
          0 \amp 0 \amp 0 \amp 0 \\
          \end{bmatrix}
        </me>.
        The null space is the set of vectors for which
        <m>T\vvec=A\vvec=\zerovec</m>, which we see
        is the subspace of <m>\real^4</m> spanned by
        <m>\fourvec0210</m> and <m>\fourvec{-2}{-1}01</m>.
        </p>

        <p>Similarly, <m>\range(T)</m> is the subspace of
        <m>\real^3</m> having a basis 
        given by <m>\threevec21{-2}</m> and
        <m>\threevec{-1}02</m>. 
        </p>
      </statement>
    </example>

    <example xml:id="example-lt-poly">
      <statement>
        <p>Suppose that <m>V=\pbb_3</m> and <m>W=\real^2</m> and that
        <m>T:V\to\real^2</m> where <m>T(p)=\twovec{p(0)}{p'(0)}</m>.
        A general polynomial in <m>V</m> has the form
        <me>
          p(x)=a_3x^3+a_2x^2+a_1x+a_0
        </me>
        so that <m>T(p)=\twovec{a_0}{a_1}</m>.  Therefore,
        <m>\nul(T)</m> is the set of polynomials for which
        <m>a_0=a_1=0</m> so that <m>p(x)=a_3x^3+a_2x^2</m>.  This
        shows that <m>\nul(T) = \laspan{x^2,x^3}</m>.</p>

        <p>
        We also
        have that
        <m>\range(T) = \real^2</m>
        </p>
      </statement>
    </example>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m>, then <m>\nul(T)</m> is a subspace of
        <m>V</m> and <m>\range(T)</m> is a subspace of <m>W</m>.
        </p>
      </statement>

      <proof>
        <p>Suppose that <m>\vvec_1</m> and <m>\vvec_2</m> are in
        <m>\nul(T)</m>.  Then we have
        <md>
          <mrow>
            T(s\vvec_1) = \amp s(T\vvec_1) = s0 = 0
          </mrow>
          <mrow>
            T(\vvec_1+\vvec_2) = \amp T(\vvec_1) + T(\vvec_2) = 0 + 0
            = 0.
          </mrow>
        </md>
        This shows that <m>\nul(T)</m> is closed under scalar
        multiplication and vector addition and is therefore a subspace
        of <m>V</m>.
        </p>
        <p>Now suppose the <m>\wvec_1</m> and <m>\wvec_2</m> are in
        <m>\range(T)</m>.  We know that there are vectors
        <m>\vvec_1</m> and <m>\vvec_2</m> in <m>V</m> such that
        <m>T(\vvec_1) = \wvec_1</m> and <m>T(\vvec_2)=\wvec_2</m>.
        Therefore,
        <md>
          <mrow>
            s\wvec_1\amp = sT(\vvec_1) = T(s\vvec_1)
          </mrow>
          <mrow>
            \wvec_1+\wvec_2 \amp = T(\vvec_1)+T(\vvec_2) =
            T(\vvec_1+\vvec_2).
          </mrow>
        </md>
        This shows that <m>\range(T)</m> is closed under scalar
        multiplication and vector addition so <m>\range(T)</m> is a
        subspace of <m>W</m>.
        </p>
      </proof>
    </proposition>

    <p>The next proposition is extremely useful so we will refer to it
    as the <q>Fundamental Theorem of Linear Maps</q>.</p>
    
    <proposition xml:id="prop-nul-range-dims">
      <title>Fundamental Theorem of Linear Maps</title>
      <statement>
        <p>If <m>V</m> is a finite dimensional vector space and
        <m>T:V\to W</m> is a linear transformation, then 
        <me>
          \dim \nul(T) + \dim \range(T) = \dim V
          </me>.
        </p>
      </statement>
      <proof>
        <p>Suppose that <m>\uvec_1,\ldots,\uvec_j</m> is a
        basis for <m>\nul(T)</m>, which we extend to a basis for
        <m>V</m> by adding vectors <m>\vvec_1,\ldots,\vvec_k</m>.
        We also define <m>\wvec_i=T\vvec_i</m>.</p>
        
        <p>Given a vector <m>\vvec</m> in <m>V</m>, we can write
        <me>
          \vvec=a_1\uvec_1 + \ldots + a_j\uvec_j + b_1\vvec_1 +
          \ldots + b_k\vvec_k
        </me>
        so that
        <md>
          <mrow>
            T\vvec\amp=a_1T\uvec_1 + \ldots + a_jT\uvec_j + b_1T\vvec_1 +
            \ldots + b_k\vvec_k
          </mrow>
          <mrow>
            \amp=b_1\wvec_1+\ldots+b_k\wvec_k
          </mrow>
          </md>.
          This shows that <m>\wvec_1,\ldots,\wvec_k</m> span
          <m>\range(T)</m>.
        </p>
        
        <p>We also claim that <m>\wvec_1,\ldots,\wvec_k</m> forms a
        linearly independent set.  Suppose that
        <md>
          <mrow>
            b_1\wvec_1 + \ldots b_k\wvec_k \amp = 0
          </mrow>
          <mrow>
            b_1T\vvec_1 + \ldots b_kT\vvec_k \amp = 0
          </mrow>
          <mrow>
            T(b_1\vvec_1+\ldots+b_k\vvec_k) \amp = 0
          </mrow>
          </md>,
          which means that <m>b_1\vvec_1+\ldots+b_k\vvec_k</m> is in
          <m>\nul(T)</m> so that this vector is a linear combination
          of <m>\uvec_1,\ldots,\uvec_j</m>.  In other words,
          <md>
            <mrow>
              b_1\vvec_1+\ldots+b_k\vvec_k \amp = a_1\uvec_1 + \ldots
              a_j\uvec_j
            </mrow>
            <mrow>
              a_1\uvec_1 + \ldots + a_j\uvec_j -
              b_1\vvec_1-\ldots-b_k\vvec_k \amp = 0
            </mrow>
          </md>.
          Since <m>\uvec_1,\ldots,\uvec_j,\vvec_1,\ldots,\vvec_k</m>
          is a basis for <m>V</m>, this set of vectors is linearly
          independent, which means that
          <me>
            a_1=\ldots=a_j=b_1=\ldots=b_k
          </me>.
          Therefore, the set of vectors <m>\basis{\wvec}{k}</m> is
          linearly independent and hence a basis for <m>\range(T)</m>.
          </p>
        
          <p>This shows that
          <me>
            \dim \nul(T) + \dim \range(T) = j + k = \dim V
          </me>.
        </p>
      </proof>
    </proposition>

    <example>
      <statement>
        <p>The Fundamental Theorem of Linear Maps is familiar from our
        earlier course where we defined the <em>rank</em> <m>r</m> of
        a matrix to be its 
        number of pivots.
        If <m>A</m> is an <m>m\times n</m> matrix and
        <m>T:\real^n\to\real^m</m> the linear transformation defined
        by <m>T(\xvec) = A\xvec</m>, we saw that
        <md>
          <mrow>
            \dim\col(A) \amp = r
          </mrow>
          <mrow>
            \dim\nul(A) \amp = n - r
          </mrow>
        </md>.
        Therefore, <m>\dim\col(A) + \dim\nul(A) = n</m>, which is an
        expression of the Fundamental Theorem of Linear Maps.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Revisiting <xref ref="example-lt-poly"/>, we had <m>T:V\to
        W</m> where <m>V=\pbb_3</m> and <m>W=\real^2</m>.
        We saw that <m>\nul(T)= \laspan{x^2,x^3}</m> so that
        <m>\dim\nul(T) = 2</m> and that <m>\range(T) = \real^2</m> so
        that <m>\dim\range(T) = 2</m>.  Therefore,
        <me>
          \dim\nul(T) + \dim\range(T) = 2 + 2 = 4 = \dim V = \dim \pbb_3
        </me>.
        </p>
      </statement>
    </example>
        
    <definition>
      <statement>
        <p>Suppose <m>T:V\to W</m>.
        <ul>
          <li>
            <p>If <m>\range(T)=W</m>, we say that
            <m>T</m> is <em>surjective</em>.  In particular, for every
            <m>\wvec</m> in <m>W</m>, there is a <m>\vvec</m> in
            <m>V</m> such that <m>T\vvec = \wvec</m>.
            </p>
          </li>
          <li>
            <p>If <m>\nul(T)=\{0\}</m>, we
            say that 
            <m>T</m> is <em>injective</em>.  In this case, if
            <m>T\vvec_1=T\vvec_2</m>, then <m>\vvec_1=\vvec_2</m>
            since <m>T(\vvec_1-\vvec_2) =
            0</m>, which means that <m>\vvec_1-\vvec_2</m> is in
            <m>\nul(T)=\{0\}</m>. 
            </p>
          </li>
        </ul>
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>Once again, these are familiar notions.  Suppose that
        <m>A</m> is an <m>m\times n</m> matrix that defines a 
        linear transformation <m>T:\real^n\to\real^m</m>.  Then
        <m>T</m> is injective if <m>\nul(A) = \{\zerovec\}</m>.  This
        happens when the columns of <m>A</m> are linearly independent.
        </p>
        <p> The transformation
        <m>T</m> is surjective if <m>\col(A) = \real^m</m>, which
        happens when the columns of <m>A</m> span <m>\real^m</m>.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>The linear transformation <m>T:\pbb\to\pbb</m> defined by
        <m>T(p)(x)=xp(x)</m> is injective but not surjective because
        the constant polynomials are not in <m>\range(T)</m>.
        </p>
      </statement>
    </example>
  

    <definition>
      <statement>
        <p>A linear transformation <m>T:V\to W</m> is called an
        <em>isomorphism</em> if <m>T</m> is both surjective and
        injective.
        </p>
      </statement>
    </definition>

    <p>The following proposition follows immediately from the <xref
    ref="prop-nul-range-dims">Fundamental Theorem of Linear Maps</xref>. 
    </p>
    
    <proposition>
      <statement>
        <p> If <m>V</m> and <m>W</m> are finite dimensional vector
        spaces and <m>T:V\to W</m>, then
        <ul>
          <li>
            <p>If <m>T</m> is surjective, then <m>\dim V \geq \dim
            W</m>.</p>
          </li>
          <li>
            <p>If <m>T</m> is injective, then <m>\dim V
            \leq \dim W</m>.</p>
          </li>
          <li>
            <p>If <m>T</m> is an isomorphism, then <m>\dim V
            = \dim W</m>.</p>
          </li>
        </ul>
        </p>
      </statement>
    </proposition>
    
  </subsection>

  <subsection>
    <title>Vector space isomorphisms</title>

    <example>
      <statement>
        <p>Consider the linear transformation <m>T:\pbb_2\to\real^3</m>
        defined by <m>T(p)=\threevec{p(0)}{p'(0)}{p''(0)}</m>.
        If <m>p(x)=a_0+a_1x+a_2x^2</m>, then
        <m>T(p) = \threevec{a_0}{a_1}{2a_2}</m>.  This shows that
        <m>\nul(T) = \{0\}</m> and <m>\range(T)=\real^3</m>.
        Therefore, <m>T</m> is a vector space isomorphism.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>If <m>V</m> is a vector space, then <m>I:V\to V</m> defined
        by <m>I(\vvec) = \vvec</m> is a
        linear transformation called the <em>identity</em>
        transformation.
        </p>
      </statement>
    </example>

    <p>Suppose that <m>T:V\to W</m> is an isomorphism, then every
    vector <m>\wvec</m> in <m>W</m> has a vector <m>\vvec</m> in
    <m>V</m> such that <m>T(\vvec) = \wvec</m>.  In fact, there is
    exactly once such vector since if <m>T(\vvec_1)=T(\vvec_2) =
    \wvec</m>, we know that <m>\vvec_1=\vvec_2</m> because <m>T</m> is
    injective.  In this case, we can define a function <m>S:W\to V</m>
    where <m>S(\wvec)</m> is the vector <m>\vvec</m> for which
    <m>T(\vvec) = \wvec</m>.
    </p>

    <p>Notice that <m>S:W\to V</m> is a linear transformation.  For
    instance, if <m>T(\vvec) = \wvec</m>, then <m>T(s\vvec) =
    s\wvec</m>, which says that
    <me>
      S(s\wvec) = s\vvec = sS(\wvec)
    </me>.
    In the same way, we have
    <me>
      S(\wvec_1+\wvec_2) = \vvec_1+\vvec_2 = S(\wvec_1)+S(\wvec_2)
    </me>.
    </p>

    <p>Therefore, we have the following proposition.
    </p>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> is an ismorphism, there is a linear
        transformation <m>S:W\to V</m> such that <m>ST=I_V</m>, the
        identity transformation on <m>V</m>, and <m>TS=I_W</m>, the
        identity transformation on <m>W</m>.  We call <m>S</m> the
        <term>inverse</term> of <m>T</m> and write
        <m>S=T^{-1}</m>.
        </p>
      </statement>
    </proposition>

    <example>
      <statement>
        <p>If <m>A</m> is an <m>m\times m</m> matrix and
        <m>T:\real^m\to\real^m</m> by <m>T(\xvec)=A\xvec</m>, then
        <m>T</m> is an isomorphism precisely when <m>A</m> is
        invertible. In this case, <m>T^{-1}(\xvec) = A^{-1}\xvec</m>.
        </p>
      </statement>
    </example>

    <proposition xml:id="prop-fin-dim-isom">
      <statement>
        <p>If <m>V</m> is a finite dimensional vector space of
        dimension <m>n</m> over the
        field <m>\field</m>, then there is an isomorphism
        <m>T:\field^n\to V</m>.
        </p>
      </statement>

      <proof>
        <p>We choose a basis <m>\basis{\vvec}{n}</m> and define
        <me>
          T\left(\cthreevec{c_1}{\vdots}{c_n}\right) = c_1\vvec_1 +
          c_2\vvec_2 + \ldots + c_n\vvec_n
        </me>.
        By the linear independence of the basis, we see that <m>T</m>
        is injective.  Since the span of the basis vectors is
        <m>V</m>, we see that <m>T</m> is surjective.
        </p>
      </proof>
    </proposition>

    <p>The term <em>isomorphism</em> means <q>having the same shape or
    structure.</q>  In other words, isomorphic vector spaces have the
    same structure.  In our earlier courses, we considered only the
    vector spaces <m>\real^n</m>.  The previous proposition, <xref
    ref="prop-fin-dim-isom"/>, shows us that every finite dimensional
    real vector space has the same structure as <m>\real^n</m>.
    This means that, technically speaking, we were also studying
    finite dimensional real vector spaces at the same time.
    </p>

    <p> Notice, however, that the 
    isomorphism in <xref ref="prop-fin-dim-isom"/> depends on a choice
    of basis.  If two people choose different bases, then they will
    produce different isomorphisms.  In fact, as we move forward, some
    of our work will be motivated by choosing a basis that creates a
    particularly nice isomorphism.  Our next discussion of matrices
    will lay that foundation.
    </p>
  </subsection>

  <subsection>
    <title>Representing linear transformations with matrices</title>

    <p><xref ref="prop-fin-dim-isom"/> says that every finite
    dimensional vector space is
    essentially the same as <m>\field^n</m>.  Therefore, we are
    able to represent elements in a vector space as more typical
    vectors in <m>\field^n</m> and linear transformations as
    matrices.  Let us make this more clear now.
    </p>

    <p>Suppose that we have a basis <m>\bcal=\{\basis{\vvec}{n}\}</m>
    for a 
    finite dimensional vector space <m>V</m>.  If <m>\vvec</m> is an
    element of <m>V</m>, then we can uniquely write
    <me>
      \vvec = c_1\vvec_1 + c_2\vvec_2 + \ldots + c_n\vvec_n
    </me>.
    As shorthand, we will write
    <me>
      \coords{\vvec}{\bcal} = \cfourvec{c_1}{c_2}{\vdots}{c_n}
    </me>.
    This should be familiar from our
    <url
        href="https://understandinglinearalgebra.org/sec-bases.html#sec-bases-4">
      earlier work
    </url> when we used a basis of <m>\real^m</m> to form a new
    coordinate system.
    </p>

    <example>
      <statement>
        <p>Consider the vector space <m>V=\pbb_2</m> with the basis
        <m>\bcal=\{1,x,x^2\}</m>.  Then we have
        <me>
          \coords{a_0+a_1x+a_2x^2}{\bcal} = \cthreevec{a_0}{a_1}{a_2}
        </me>.
        We may think of this as a coordinate system in the vector
        space of polynomials.
        </p>
      </statement>
    </example>

    <p>In a similar way, we can represent linear transformations using
    matrices.  
    Suppose that <m>T:V\to W</m> is a linear transformation and that
    we have a basis <m>\bcal=\{\vvec_1,\ldots,\vvec_n\}</m> for
    <m>V</m> and a basis <m>\ccal=\{\wvec_1,\ldots,\wvec_m\}</m> for
    <m>W</m>.  We then have
    <me>
      T(\vvec_j) = A_{1,j}\wvec_1 + A_{2,j}\wvec_2 + \ldots +
      A_{m,j}\wvec_m 
    </me>,
    which defines an <m>m\times n</m> matrix <m>A</m>.  In the same
    way we denoted the coordinates of a vector in terms of a basis, we
    can denote the matrix of the linear transformation
    <m>\coords{T}{\ccal,\bcal} = A</m>.  Notice that
    <me>
      \coords{T(\vvec_j)}{\ccal} =
      \cfourvec{A_{1,j}}{A_{2,j}}{\vdots}{A_{n,j}}
    </me>
    meaning that the columns of <m>\coords{T}{\ccal,\bcal}</m> are the
    coordinates <m>\coords{T(\vvec_j)}{\ccal}</m>:
    <me>
      \coords{T}{\ccal,\bcal} =
      \left[
      \begin{array}{cccc}
      \coords{T(\vvec_1)}{\ccal} \amp
      \coords{T(\vvec_2)}{\ccal} \amp
      \ldots
      \coords{T(\vvec_m)}{\ccal}
      \end{array}
      \right]
    </me>.
    </p>

    <p>At first glance, this notation may seem a little intimidating,
    but it will become clear with a little practice.
    </p>

    <definition>
      <statement>
        <p>If <m>T:V\to W</m> is a linear transformation, <m>\bcal</m>
        a basis for <m>V</m> and <m>\ccal</m> a basis for <m>W</m>, we
        say that the matrix <m>\coords{T}{\ccal,\bcal}</m> is the matrix
        associated to <m>T</m> with respect to these bases.  
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>Consider <m>T:\pbb_3\to\pbb_2</m> where <m>T(p)=p'</m>.  If
        we choose the bases <m>\bcal=\{x^3,x^2,x,1\}</m> and
        <m>\ccal=\{x^2,x,1\}</m>, then
        <me>
          \coords{T}{\ccal,\bcal}=\begin{bmatrix}
          3 \amp 0 \amp 0 \amp 0 \\
          0 \amp 2 \amp 0 \amp 0 \\
          0 \amp 0 \amp 1 \amp 0 \\
          \end{bmatrix}
        </me>.
        </p>
      </statement>
    </example>

    <!--
    <proposition>
      <statement>
        <p>If <m>\bcal</m> and <m>\ccal</m> are
        bases for <m>V</m> and <m>W</m> respectively, then
        the map that assigns <m>T\to M_T^{\ccal,\bcal}</m> is a linear
        transformation.  More specifically,
        <md>
          <mrow>
            M_{cT}^{\ccal,\bcal} \amp = cM_T^{\ccal,\bcal}
          </mrow>
          <mrow>
            M_{T+S}^{\ccal,\bcal} \amp = M_T^{\ccal,\bcal} +
            M_S^{\ccal,\bcal}.
          </mrow>
        </md>
        </p>
      </statement>
    </proposition>
    -->

    <p> The next proposition says that the composition of linear
    transformations corresponds to matrix multiplication.</p>

    <proposition>
      <statement>
        <p>If <m>T:V\to W</m> and <m>S:W\to U</m> are linear
        transformations and <m>\bcal</m>, <m>\ccal</m>, and
        <m>\dcal</m> are bases for <m>V</m>, <m>W</m>, and <m>U</m>,
        respectively, then
        <me>
          \coords{ST}{\dcal, \bcal} = \coords{S}{\dcal,\ccal}
          \coords{T}{\ccal,\bcal} 
        </me>.
        </p>
      </statement>

      <proof>
        <p>We denote the vectors in the bases by
        <m>\basis{\vvec}{m}</m>, <m>\basis{\wvec}{n}</m>, and
        <m>\basis{\uvec}{p}</m>, respectively.  Similarly, we use the
        shorthand 
        <md>
          <mrow>
            A \amp = \coords{T}{\ccal,\bcal}
          </mrow>
          <mrow>
            B \amp = \coords{S}{\dcal,\ccal}
          </mrow>
          <mrow>
            C \amp = \coords{ST}{\dcal,\bcal}
          </mrow>
        </md>
        </p>
        <p> We have
        <md>
          <mrow>
            T\vvec_j \amp = \sum_k A_{k,j} \wvec_k
          </mrow>
          <mrow>
            S\wvec_k \amp = \sum_l B_{l,k} \uvec_l
          </mrow>
        </md>,
        which implies that
        <md>
          <mrow>
            (ST)\vvec_j \amp = S\left(\sum_kA_{k,j}\wvec_k\right)
          </mrow>
          <mrow>
            \amp = \sum_k A_{k,j} S(\wvec_k)
          </mrow>
          <mrow>
            \amp = \sum_k A_{k,j} \sum_l B_{l,k}\uvec_l
          </mrow>
          <mrow>
            \amp = \sum_l \sum_k B_{l,k}A_{k,j} \uvec_l
          </mrow>
          <mrow>
            \amp = \sum_l C_{l,j} \uvec_l.
          </mrow>
        </md>
        Therefore, <m>C_{l,j} = \sum_kB_{l,k}A_{k,j}</m>, which says
        that <m>C=BA</m> as expected.
        </p>
      </proof>
          
    </proposition>

    <p>A similar result holds for the coordinate representations of
    vectors.
    </p>
    <proposition>
      <statement>
        <p>Suppose that <m>T:V\to W</m> is a linear transformation and
        <m>\bcal</m> is a basis for <m>V</m> and <m>\ccal</m> is a
        basis for <m>W</m>.  If <m>\vvec</m> is a vector in <m>V</m>,
        then
        <me>
          \coords{T(\vvec)}{\ccal} = \coords{T}{\ccal,\bcal}
          \coords{v}{\bcal}
        </me>.
        </p>
      </statement>
    </proposition>

    <p>An important example is when the linear transformation is
    the identity <m>I:V\to V</m> and we have two bases
    <m>\bcal</m> and <m>\ccal</m> for <m>V</m>.  In this case,
    <me>
      \coords{I}{\ccal,\bcal} =
      \left[
      \begin{array}{ccc}
      \coords{\vvec_1}{\ccal} \amp \ldots \amp
      \coords{\vvec_n}{\ccal}
      \end{array}
      \right]
    </me>.
    This matrix then represents the change of coordinates
    <me>
      \coords{\vvec}{\ccal} = \coords{I}{\ccal,\bcal}
      \coords{\vvec}{\bcal}
    </me>.
    </p>

    <example>
      <statement>
        <p>Suppose that <m>V = \pbb_2</m> and that
        <m>\bcal=\{1+x,x-x^2,x^2-1\}</m> and
        <m>\ccal=\{1,x,x^2\}</m>.  Then
        <md>
          <mrow>
            \coords{I}{\ccal,\bcal}\amp =
            \begin{bmatrix}
            \coords{1+x}{\ccal} \amp
            \coords{x-x^2}{\ccal} \amp
            \coords{x^2-1}{\ccal}
            \end{bmatrix}
          </mrow>
          <mrow>
            \amp =
            \begin{bmatrix}
            1 \amp 0 \amp -1 \\
            1 \amp 1 \amp 0 \\
            0 \amp -1 \amp 1
            \end{bmatrix}.
          </mrow>
        </md>
        This matrix converts the coordinate representation of a
        polynomials in the <m>\bcal</m> basis into the coordinate
        representation of the same polynomial in the <m>\ccal</m>
        basis. 
        </p>

        <p>The inverse of this matrix will convert the
        <m>\ccal</m>-coordinate representation of a polynomial into
        the <m>\bcal</m>-coordinate representation:
        <me>
          \coords{I}{\bcal,\ccal} = \coords{I}{\ccal,\bcal}^{-1} =
          \begin{bmatrix}
          1/2 \amp 1/2 \amp 1/2 \\
          -1/2 \amp 1/2 \amp -1/2 \\
          -1/2 \amp 1/2 \amp 1/2 \\
          \end{bmatrix}
        </me>.
        </p>

        <p>Consider the polynomial <m>p(x)=4-2x+8x^2</m>.  We then
        have
        <me>
          \coords{p}{\ccal} = \threevec4{-2}8
        </me>
        and
        <me>
          \coords{p}{\bcal} = 
          \begin{bmatrix}
          1/2 \amp 1/2 \amp 1/2 \\
          -1/2 \amp 1/2 \amp -1/2 \\
          -1/2 \amp 1/2 \amp 1/2 \\
          \end{bmatrix}
          \threevec4{-2}8
          = \threevec5{-7}1
        </me>.
        This means that
        <me>
          p(x) = 4-2x+8x^2 = 5(x+1)-7(x-x^2)+1(x^2-1)
        </me>
        as is easily checked.
        </p>
      </statement>
    </example>

    <p>We will often be interested in linear transformations <m>T:V\to
    V</m> in which the codomain and the domain are the same vector
    space.  In this case, we say that <m>T</m> is an <em>operator</em> on
    <m>V</m>. </p>

    <definition xml:id="definition-operator">
      <statement>
        <p>A linear transformation <m>T:V\to V</m> is called an
        <term>operator</term> on <m>V</m>.</p>
      </statement>
    </definition>

    <p>Given a basis <m>\bcal</m> for <m>V</m>, we can then
    represent <m>T</m> in terms of this basis as
    <m>\coords{T}{\bcal,\bcal}</m> where the same basis is used for
    the codomain and domain.
    The following shows how the matrices representing the same
    transformation are represented in two bases.
    </p>

    <proposition xml:id="prop-matrix-change-basis">
      <statement>
        <p>Suppose that <m>T:V\to V</m> is a linear transformation and
        that <m>\bcal</m> and <m>\ccal</m> are two bases for
        <m>V</m>.  Then
        <me>
          \coords{T}{\ccal,\ccal} =
          \coords{I}{\ccal,\bcal}
          \coords{T}{\bcal,\bcal}
          \coords{I}{\bcal,\ccal}
          =
          \coords{I}{\ccal,\bcal}
          \coords{T}{\bcal,\bcal}
          \coords{I}{\ccal,\bcal}^{-1}
        </me>.
        </p>
      </statement>
    </proposition>

    <p>Here is a simpler way to represent this statement.  If <m>B =
    \coords{T}{\ccal,\ccal}</m>, <m>A=\coords{T}{\bcal,\bcal}</m>, and
    <m>P=\coords{I}{\ccal,\bcal}</m>, then we have
    <me>
      B = PAP^{-1}
    </me>.
    This should remind you of the kind of expression we saw when we
    were diagonalizing matrices and gives some idea for where we are
    heading.  In particular, given an operator <m>T</m> on a vector
    space <m>V</m>, we would like to find a basis <m>\bcal</m> for
    <m>V</m> so that <m>\coords{T}{\bcal,\bcal}</m> is relatively
    simple, such as a diagonal matrix.  The expression in <xref
    ref="prop-matrix-change-basis"/>
    can help us do that, even though that expression may look
    daunting. </p>

    <definition>
      <statement>
        <p>Two square <m>n\times n</m> matrices <m>A</m> and <m>B</m>
        are called 
        <term>similar</term> if there is a matrix <m>P</m> such that
        <me>
          B = PAP^{-1}
        </me>.
        </p>
      </statement>
    </definition>

    <p>Notice that a matrix is diagonalizable precisely when it is
    similar to a diagonal matrix.
    </p>

    <proposition>
      <statement>
        <p>Similarity is an equivalence relation on the set of
        <m>n\times n</m> matrices.
        </p>
      </statement>
    </proposition>

    <proposition>
      <statement>
        <p>Suppose that <m>A</m> and <m>B</m> are similar <m>n\times
        n</m> matrices.  If <m>\bcal</m> is a basis for <m>V</m> and
        <m>A=\coords{T}{\bcal,\bcal}</m>,
        then <m>B=\coords{T}{\ccal,\ccal}</m> for
        some other basis <m>\ccal</m>.
        </p>
      </statement>
    </proposition>

    <p>In other words, two similar matrices represent the same linear
    transformations in two different bases.  This is why we should
    expect that similar matrices should share important properties,
    such as determinants, eigenvalues, and more.
    </p>

    <p>If <m>V</m> and <m>W</m> are vector spaces, remember that
    <xref ref="definition-l-v-w"/> defined
    <m>L(V,W)</m> to be the set of linear transformations <m>T:V\to
    W</m>.  We now note that <m>L(V,W)</m> is itself a vector space.
    </p>

    <proposition xml:id="prop-lt-vs">
      <statement>
        <p>If <m>V</m> and <m>W</m> are two vector spaces, then
        <m>L(V,W)</m> is a vector space.  Moreover, if <m>\bcal</m> is
        a finite basis for <m>V</m> and <m>\ccal</m> is a finite basis
        for <m>W</m>, then the function <m>S:L(V,W)\to
        \field^{m,n}</m>, the vector space of <m>m\times n</m>
        matrices, by
        <me>
          S(T) = \coords{T}{\ccal,\bcal}
        </me>
        is an isomorphism. It then follows that
        <me>
          \dim L(V,W) = \dim V \dim W
        </me>.
        </p>
      </statement>
    </proposition>
      
    
  </subsection>

  <subsection>
    <title>Dual spaces</title>

    <p>A special case of the vector space <m>L(V,W)</m> of linear
    transformations from <m>V</m> to <m>W</m> is when <m>W=\field</m>.
    In this case, we call <m>L(V,\field)</m> the <em>dual</em> of
    <m>V</m>.
    </p>

    <definition>
      <statement>
        <p>The vector space of linear transformations
        <m>L(V,\field)</m> is called the <term>dual</term> of <m>V</m>
        and is denoted by
        <me>
          V'=L(V,\field)
        </me>.
        An element of <m>V'</m> is called a <term>functional</term> on
        <m>V</m>. 
        </p>
      </statement>
    </definition>

    <p>Up to this point, we have typically used <m>T</m> to denote a
    linear transformation.  However, we often use <m>\phi</m> to
    represent an element of the dual.</p>

    <example>
      <statment>
        <p>Suppose that <m>V=\real^3</m>.  Then
        <me>\phi\left(\threevec xyz\right) = 2x + 3y -z
        </me>
        is a functional on <m>V</m> and hence an element of the dual
        <m>(\real^3)'</m>.
        </p>

        <p>Notice that we can represent this functional in terms of
        the dot product if we define the vector
        <m>\uvec=\threevec23{-1}</m>.  Then
        <me>
          \phi(\vvec) = \uvec\cdot\vvec
        </me>.
        We will later understand this example as an illustration of
        the <xref ref="prop-riesz">Riesz Representation
        Theorem</xref>.  More generally, any vector <m>\uvec</m> in
        <m>\real^3</m> defines a functional by
        <m>\phi(\vvec)=\uvec\cdot\vvec</m>. 
        </p>
      </statment>
    </example>

    <example>
      <statement>
        <p>If <m>V=\pbb_3</m>, then <m>\phi(p)=p(0)</m> is a
        functional on <m>V</m>.
        </p>
      </statement>
    </example>

    <p>By <xref ref="prop-lt-vs"/>, we know that <m>V'</m> is a vector
    space and that <m>\dim V' = \dim V</m>.  Therefore, if <m>\dim
    V=n</m>, we see that <m>\dim V' = n</m> as well and given a
    basis for <m>V</m>, we can construct an explicit isomorphism
    <m>T:V'\to \real^n </m>.
    </p>

    <p>Suppose that <m>\basis{\vvec}{n}</m> is a basis for <m>V</m> and
    that <m>\phi</m> is a functional on <m>V</m>.  If <m>\vvec</m> is
    in <m>V</m>, then we can write
    <me>
      \vvec = a_1\vvec_1 + a_2\vvec_2 + \ldots + a_n\vvec_n
    </me>
    and hence
    <me>
      \phi(\vvec) = a_1\phi(\vvec_1) + a_2\phi(\vvec_2) + \ldots +
      a_n\phi(\vvec_n)
    </me>.
    This shows that <m>\phi</m> is determined by the vector
    <m>\cfourvec{\phi(\vvec_1)}{\phi(\vvec_2)}{\vdots}{\phi(\vvec_n)}</m>.
    We therefore define the isomorphism
    <m>T:V'\to \real^n</m> by
    <me>
      T(\phi) =
      \cfourvec{\phi(\vvec_1)}{\phi(\vvec_2)}{\vdots}{\phi(\vvec_n)}
    </me>.
    We see that <m>T</m> is an isomorphism because it is injective:
    if <m>T(\phi) = \zerovec</m>, then <m>\phi(\vvec_j)=0</m> for all
    <m>j</m> and hence
    <me>
      \phi(\vvec) = a_1\phi(\vvec_1) + a_2\phi(\vvec_2) + \ldots +
      a_n\phi(\vvec_n) = 0
    </me>
    for all vectors <m>\vvec</m>.  Since <m>T</m> is injective and
    <m>\dim V'=\dim \real^n</m>, <m>T</m> is also surjective by <xref 
    ref="prop-nul-range-dims">Fundamental Theorem of Linear Maps</xref>.       
    </p>

    <proposition>
      <statement>
        <p>If <m>V</m> is a finite dimensional vector space, the dual
        space <m>V'</m> satisfies
        <me>
          \dim V' = \dim V
        </me>
        and an isomorphism is given once we choose a basis for
        <m>V</m>.
        </p>
      </statement>
    </proposition>

    <example>
      <statement>
        <p>Consider <m>V=\pbb_2</m> with basis <m>1,x,x^2</m>.  Define
        functionals <m>\phi_1</m>, <m>\phi_2</m>, and <m>\phi_3</m> by
        <md>
          <mrow>
            \phi_1(1) = 1, \phi_1(x) = 0, \phi_1(x^2) =0
          </mrow>
          <mrow>
            \phi_2(1) = 0, \phi_2(x) = 1, \phi_2(x^2) =0
          </mrow>
          <mrow>
            \phi_3(1) = 0, \phi_3(x) = 0, \phi_3(x^2) =1
          </mrow>
        </md>.
        Then any functional <m>\phi</m> with
        <me>
          \phi(1) = a_1, \phi(x)=a_2, \phi(x^2)=a_3
        </me>
        satisfies <m>\phi=a_1\phi_1 + a_2\phi_2+a_3\phi_3</m>.
        </p>
      </statement>
    </example>
    
  </subsection>

      
</section>
