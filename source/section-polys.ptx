<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-polys" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>The minimal polynomial</title>

  <introduction>
    <p> This section will describe some important properties of
    polynomials and demonstrate how they might help us understand linear
    transformations or, more accurately,
    <xref ref="definition-operator"
          text="custom">operators,</xref> which are linear
    transformations <m>T:V\to V</m> from a vector space to itself.
    </p>
    
    <p> We will begin by studying familiar polynomials, such as you
    saw in high school algebra.  For example, <m>p(x)=2x^2 - 3x + 7</m>
    is a quadratic polynomial.  Eventually, we will consider polynomials
    evaluated on an operator <m>T</m>.  For instance,
    evaluating <m>p(x)</m> on the operator <m>T</m> produces a new
    operator
    <m>p(T) = 2T^2 - 3T + 7I</m>.  As a preview for how this may be
    useful, the polynomial <m>p(x)=x-2</m> leads to the operator
    <m>p(T) = T-2I</m> whose null space is the set of eigenvectors
    associated to the eigenvalue <m>\lambda = 2</m>.
    </p>
  </introduction>
  
  <subsection>
    <title>Properties of Polynomials</title>

    <p>We will be interested in polynomials that depend on a single
    variable, which will often be <m>x</m>.  
    A general polynomial has
    the form
    <me>
      p(x)=a_nx^n + a_{n-1}x^{n-1}+\ldots + a_1x+a_0
    </me>
    where <m>a_j</m> are the <em>coefficients</em> of <m>p(x)</m> and
    are elements in either <m>\field=\C</m> or
    <m>\real</m>.
    </p>

    <definition xml:id="definition-degree">
      <statement>
        <p>The <term>degree</term> of a polynomial is the highest
        power of <m>x</m> whose coefficient is nonzero.  For example,
        the degree of 
        <me>
          p(x)=-2x^7 + 4x^6 -x^5 + 4x^3-3x + 1
        </me>
        is <m>\deg(p) = 7</m>.
        </p>
      </statement>
    </definition>

    <p> We have already seen that the set of polynomials form a vector
    space, which means we can multiply polynomials by scalars
    and we can add them.  An additional operation allows us to
    multiply two polynomials 
    together.
    </p>

    <example xml:id="example-poly-product">
      <statement>
        <p>Suppose that <m>p(x)=3x^2-4x+2</m> and <m>q(x)=8x-5</m>.
        We multiply polynomials term by term so that
        <me>
	  (pq)(x) = 24x^3-27x^2+36x-10
        </me>
        More generally, denoting the coefficients of <m>p</m> by
        <m>a_j</m> and the coefficients of <m>q</m> by <m>b_j</m>, the
        coefficients of <m>pq</m> are
        <me xml:id="me-poly-mult">
	  c_k=\sum_{j=0}^k a_jb_{k-j}=\sum_{j=0}^k b_ja_{k-j}
          </me>.
        </p>
      </statement>
    </example>

    <assemblage>
      <title>Properties of polynomial multiplication</title>
      <p>Two important properties follow from the general expression
      for the coefficients of the product given in
      <xref ref="example-poly-product"/>.
      <ul>
        <li>
          <p>The order in which we multiply
          polynomials is irrelevant;  that is,
          <me>pq = qp</me>.
          </p>
        </li>

        <li>
          <p>The degree of the product is the sum of the degrees;
          that is,
          <me>
            \deg(pq)=\deg(p)+\deg(q)
          </me>.
          </p>
        </li>
      </ul>
      </p>
    </assemblage>
      
    <p>Integers satisfy a property often called the <em>Division
    Algorithm</em>.  Suppose that <m>a</m> and <m>b</m> are positive
    integers.  Then there are integers <m>q</m> and <m>r</m> such that
    <me>
      a = bq + r
    </me>
    where <m>0\leq r \leq b-1</m>.  We say that <m>q</m> is the
    <em>quotient</em> and <m>r</m> is the <em>remainder</em>.  For
    example, if 
    <m>a=17</m> and <m>b=5</m>, then
    <me>
      17 = 5\cdot3 + 2
    </me>.
    We might say <q>17 divided by 5 has a quotient of 3 and a
    remainder of 2.</q>
    </p>

    <p>Polynomials satisfy a similar property also called the Division
    Algorithm as given in the following proposition.
    </p>

    <proposition xml:id="prop-poly-division">
      <title>The Division Algorithm</title>
      <statement>
	<p>If <m>p</m> and <m>s</m> are polynomials, then there
	are unique polynomials <m>q</m> and <m>r</m> such that
	<me>
	  p = qs + r
	</me>
	where <m>\deg(r) \lt \deg(s)</m>.
	</p>
      </statement>
    </proposition>
    
    <example>
      <statement>
	<p>Rather than proving the Division Algorithm for polynomials,
	we will illustrate with an example.  Suppose that
	<m>p(x)=6x^3 + x^2 - 17x + 8</m> and <m>s(x)=2x^2 + 2x -
	1</m>.
	</p>

	<p>To get started, let's consider the highest degree terms in
	both polynomials.  For <m>p(x)</m>, it is <m>6x^3</m> while
	for <m>s(x)</m>, it is <m>2x^2</m>.  We'd like to multiply
	<m>s(x)</m> by something to get close to <m>p(x)</m>. Since
	the ratio of the highest degree terms is <m>3x</m>, we
	multiply <m>s(x)</m> by <m>3x</m>, which gives
	<me>
	  p(x) - (3x)s(x) = -8x^2 - 14x + 8
	</me>.
	</p>

	<p>Now we ask what we should multiply by <m>s(x)</m> to
	realize the <m>-8x^2</m> term.  Since the highest degree term of
	<m>s(x)</m> is <m>2x^2</m>, we multiply by <m>-4</m>.  This
	means that
	<me>
	  p(x) - (3x-4)s(x) = -2x+4
	</me>.
	Since the highest degree term of <m>s(x)</m> is <m>2x^2</m>,
	there is nothing we can multiply by <m>s(x)</m> to realize the
	highest degree term of the right hand side.
	This means that we have
	<me>
	  p(x) = (3x-4)s(x) + (-2x+4)
	</me>
	so the quotient is <m>q(x)=3x-4</m> and the remainder is
	<m>r(x)=-2x+4</m>. 
	</p>

	<p>This algorithm is sometimes called <em>synthetic
	division</em> and is sometimes taught in high school.  We
	won't be actually be implementing this algorithm, but this
	example is meant to illustrate why <xref
	ref="prop-poly-division"/> is true.  Implementing this
	algorithm for general polynomials will give a proof of the
	Division Algorithm.
	</p>
      </statement>
    </example>

    <p>One important implication of the Division Algorithm is the
    following fact that we will use quite a bit.  Remember that a root
    <m>\lambda</m> of a polynomial is a value for which
    <m>p(\lambda)=0</m>. 
    </p>

    <proposition xml:id="prop-root-factor">
      <statement> If <m>\lambda</m> is a root of the polynomial <m>p(x)</m>,
      then
      <me>
	p(x)=(x-\lambda)q(x)
      </me>
      for some polynomial <m>q</m>.
      </statement>

      <proof>
	<p>The Division Algorithm tells us that
	<me>p(x)=(x-\lambda)q(x) + r(x)
	</me>.
	In the notation of the Division Algorithm,
	<m>s(x)=x-\lambda</m> and <m>\deg(r) \lt \deg(s) = 1</m>.
	Therefore <m>\deg(r)=0</m>, which means that <m>r(x)</m> is a
	constant <m>r</m>.
	</p>

	<p>Now we know that <m>0=p(\lambda)=(\lambda-\lambda)q(\lambda)+r =
	r</m>, which says that <m>r=0</m>.  Therefore, we have
	<me>
	  p(x)=(x-\lambda)q(x)
	</me>.
	</p>
      </proof>
    </proposition>

    <p>This proposition shows that there is a relationship between the
    factors of a polynomial <m>p(x)</m> and its roots.  Polynomials
    with <em>complex</em> coefficients are special because the Fundamental
    Theorem of Algebra tells us that every such polynomial has a
    root.  The proof of this theorem is outside the scope of our
    course, but is is usually included in a course in complex analysis.
    </p>

    <theorem xml:id="thm-fta">
      <title>Fundamental Theorem of Algebra</title>

      <statement>
	<p>If <m>p(x)</m> is a degree <m>n</m> polynomial having
	complex coefficients, then there are <m>n</m> roots,
	<m>\lambda_1, \lambda_2, \ldots, \lambda_n</m>, possibly with
	repetition.  This means that
	<me>p(x)=a_n(x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_n)
	</me>.
	</p>
      </statement>
    </theorem>

    <p>While this important theorem only holds for polynomials with
    complex coefficients, we can deduce something about polynomials
    with real coefficients.  For instance, if <m>p(x)</m> is a
    polynomial with real coefficients and <m>\lambda</m> is a complex,
    non-real 
    root, then so is its complex conjugate <m>\overline{\lambda}</m>.
    This follows because
    <me>
      0=\overline{p(\lambda)} = p(\overline{\lambda})
    </me>.
    We have <m>(x-\lambda)(x-\overline{\lambda}) = x^2+bx + c</m> where
    <m>b = -(\lambda+\overline{\lambda})</m> and
    <m>c=\lambda\overline{\lambda}</m>.  Since this quadratic
    polynomial does not have real roots, we know from the quadratic
    formula that <m>b^2\lt 4c</m>.  Therefore,
    </p>

    <proposition>
      <statement>
	<p>If <m>p(x)</m> is a polynomial with real coefficients,
	there are real roots <m>\lambda_1,\ldots,\lambda_m</m> such
	that 
	<me>
	  p(x)=a_n(x-\lambda_1)\ldots(x-\lambda_m)
          (x^2+b_1x+c_1^2)\ldots(x^2+b_sx+c_s)
	</me>
        where <m>b_i^2 \lt 4c_i</m> for <m>1\leq i\leq k</m>.
	</p>
      </statement>
    </proposition>

    <p>As mentioned in the introduction to this section, our interest
    in polynomials stems from the insights we gain when we evaluate a
    polynomial on an operator <m>T</m>.  For instance, if
    <m>p(x)=x^2+4x-5</m>, then <m>p(T) = T^2 + 4T - 5I</m> a new
    operator.  Notice 
    that we consider the constant term to be multiplied by the
    identify transformation <m>I</m>.
    </p>

    <example>
      <statement>
        <p>Consider the operator <m>T:\real^2\to\real^2</m> by <m>T(\xvec)
        = A\xvec</m> where
        <m>A=\begin{bmatrix}
        2 \amp -1 \\
        2 \amp 3
        \end{bmatrix}
        </m>.  If <m>p(x)=x^2+4x-5</m>, then
        <me>
          p(T)\xvec = (A^2 + 4A -5I)\xvec =
          \begin{bmatrix}
          5 \amp -9 \\
          18 \amp 14 \\
          \end{bmatrix}
          \xvec
        </me>.
        This means that <m>p(T):\real^2\to\real^2</m> is the operator
        represented by the matrix 
        <m>
          \begin{bmatrix}
          5 \amp -9 \\
          18 \amp 14 \\
          \end{bmatrix}
        </m>.
        </p>
      </statement>
    </example>

  </subsection>

  <subsection>
    <title>Invariant subspaces</title>

    <p>As we begin to study operators, the concept of an
    <em>invariant</em> subspace will be helpful.  In particular,
    finding invariant subspaces will frequently help us reduce our
    focus from an entire vector space to a smaller subspace so that we
    can more easily draw conclusions.</p>

    <definition>
      <statement>
        <p>If <m>W</m> is a vector subspace of <m>V</m> and <m>T:V\to
        V</m> an operator, we say that <m>W</m> is <em>invariant</em>
        under <m>T</m> if <m>T(\wvec)\in W</m> for all <m>\wvec\in
        W</m>.  We frequently say that <m>W</m> is <m>T</m>-invariant.
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>If <m>T</m> is an operator and <m>T\vvec =
        \lambda\vvec</m>, we say that <m>\vvec</m> is an
        <term>eigenvector</term> of <m>T</m> with associated
        <term>eigenvalue</term> <m>\lambda</m>.  This is the same
        concept that we explored with matrices.
        </p>

        <p>The eigensapce associated to the eigenvalue <m>\lambda</m>,
        <m>W=E_\lambda</m>, which consists of all the eigenvectors of
        <m>T</m> is a <m>T</m>-invariant subspace.  In particular, if
        <m>\vvec</m> is in <m>E_\lambda</m>, then <m>T\vvec =
        \lambda\vvec</m>, which is also in <m>E_\lambda</m>.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Consider the operator <m>T:\real^3\to\real^3</m>, which
        rotates vectors by <m>90^\circ</m> around the <m>z</m>-axis.
        The <m>z</m>-axis forms an invariant subspace since it is
        equal to <m>E_1</m>.
        </p>

        <p>Moreover, the set of vectors <m>\threevec xy0</m> forms a
        two-dimensional <m>T</m>-invariant subspace since any vector
        in that plane is rotated to another vector in the same plane.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>V=\pbb_8</m>, the vector space of
        polynomials of degree 8 or less, and that <m>T:V\to V</m> by
        <m>T(p) = p'</m>.  Then <m>W=\pbb_5</m> is a
        <m>T</m>-invariant subspace because the derivative of any
        degree five polynomial has a degree less than five.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Suppose that <m>T:V\to V</m> is an operator.  Then
        <m>\nul(T)</m> is a <m>T</m>-invariant subspace.  For example,
        if <m>\vvec</m> is in <m>\nul(T)</m>, then <m>T\vvec=0</m>,
        which is also in <m>\nul(T)</m>.
        </p>

        <p>Furthermore, <m>\range(T)</m> is also <m>T</m>-invariant.
        If <m>\wvec\in\range(T)</m>, then <m>T\wvec</m> is in
        <m>\range(T)</m> by the definition of <m>\range(T)</m>.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>Any operator <m>T:V\to V</m> will have two invariant
        subspaces, namely <m>\{0\}</m> and <m>V</m> itself.
        </p>
      </statement>
    </example>

    <p>Remember that the order in which we multiply polynomials is not
    important.  In particular, <m>p(T)q(T)=q(T)p(T)</m>.  This leads
    to the following important proposition.
    </p>

    <proposition xml:id="prop-poly-invariant-subspaces">
      <statement>
	<p>If <m>p(x)</m> is a polynomial and <m>T</m> an operator on
        a vector space <m>V</m>,
	then both <m>\nul(p(T))</m> and <m>\range(p(T))</m> are
	<m>T</m>-invariant subspaces of <m>V</m>.
	</p>
      </statement>

      <proof>
        <p>Consider the polynomial <m>q(x)=x</m> so that
        <m>(pq)(x)=xp(x)=p(x)x</m>.  This means that
        <m>p(T)T=Tp(T)</m>.</p>

	<p> Suppose that <m>\vvec</m> is in <m>\nul(p(T))</m>, which
	means that <m>p(T)\vvec = \zerovec</m>.  We need to explain
	why <m>T\vvec</m> is also in <m>\nul(p(T))</m>, which leads us
        to 
	<me>
	  p(T)T\vvec = Tp(T)\vvec = T\zerovec = \zerovec
	</me>.
	</p>

	<p>Similarly, if <m>\vvec</m> is in <m>\range(p(T))</m>, then
	there is a vector <m>\uvec</m> so that
	<m>\vvec=p(T)\uvec</m>.  Then
	<me>
	  T\vvec = Tp(T)\uvec = p(T)(T\uvec)
	</me>,
	which shows that <m>T\vvec</m> is also in
	<m>\range(p(T))</m>.  Therefore, <m>\range(p(T))</m> is an
	invariant subspace of <m>T</m>.
	</p>
      </proof>
    </proposition>

    <example xml:id="example-nul-range-invariant">
      <statement>
        <p>Consider the operator <m>T:\real^4\to\real^4</m> by
        <m>T\xvec = A\xvec</m> with
        <m>A=\begin{bmatrix}
        -10 \amp 16 \amp 5 \amp -9 \\
        -6 \amp 10 \amp 1 \amp -5 \\
        -2 \amp 4 \amp 1 \amp -5 \\
        2 \amp -4 \amp -3 \amp 3 \\
        \end{bmatrix}
        </m>.
        Consider also the polynomial <m>p(x) = x^2+4x+4</m> so that
        the operator <m>p(T)</m> is represented by the matrix
        <me>
          B = A^2 + 4A+4I =
          \begin{bmatrix}
          -60 \amp 120 \amp 18 \amp -78 \\
          -36 \amp 72 \amp 0 \amp -36 \\
          -24 \amp 48 \amp 18 \amp -42 \\
          24 \amp -48 \amp -18 \amp 42
          \end{bmatrix}
        </me>.
        </p>

        <p>Notice that the reduced row echelon form of <m>B</m> is
        <me>
          B\sim\begin{bmatrix}
          1 \amp -2 \amp 0 \amp 1 \\
          0 \amp 0 \amp 1 \amp -1 \\
          0 \amp 0 \amp 0 \amp 0 \\
          0 \amp 0 \amp 0 \amp 0 \\
          \end{bmatrix}
        </me>.
        First considering <m>\range(p(T))=\col(B)</m>, we find a basis
        consisting of the first and third columns of <m>B</m>:
        <me>
          \wvec_1 = \fourvec{-60}{-36}{-24}{24},\hspace{12pt}
          \wvec_2 = \fourvec{18}{0}{18}{18}
        </me>.
        We can check that
        <md>
          <mrow>
            T\wvec_1 \amp = 4\wvec_1 + 4\wvec_2
          </mrow>
          <mrow>
            T\wvec_2 \amp = 4\wvec_2
          </mrow>
        </md>.
        This shows that <m>\range(p(T))</m> is <m>T</m>-invariant
        since <m>T\wvec_1</m> and <m>T\wvec_2</m> lie in
        <m>\range(p(T))</m>.
        </p>

        <p>We also have a basis for <m>\nul(p(T))=\nul(B)</m>
        consisting of vectors
        <me>
          \uvec_1 = \fourvec{2}{1}{0}{0},\hspace{12pt}
          \uvec_2 = \fourvec{-1}{0}{1}{1}
        </me>.
        We can check that
        <md>
          <mrow>
            T\uvec_1 \amp = -2\uvec_1
          </mrow>
          <mrow>
            T\uvec_2 \amp = -2\uvec_1-2\uvec_2
          </mrow>
        </md>.
        Once again, this shows that <m>\nul(p(T))</m> is
        <m>T</m>-invariant.
        </p>
      </statement>
    </example>
          
  </subsection>

  <subsection>
    <title>The minimal polynomial of a vector</title>

    <p>We are approaching a result that will be crucial for our
    upcoming explorations. 
    First, we need to make the following definition clear.
    </p>

    <definition>
      <statement>A <em>monic</em> polynomial is a polynomial where the
      coefficient of the highest degree term is <m>1</m>.
      </statement>
    </definition>

    <p>For instance, <m>x^5 - x^4 + 3x - 2</m> is a monic polynomial,
    but <m>3x^3+2x-1</m> is not.
    </p>

    <theorem xml:id="thm-minimal-polynomial-vector">
      <title>The minimal polynomial of an operator applied to a
      vector</title>

      <statement>
        <p>Suppose that <m>T:V\to V</m> is an operator on a finite
        dimensional vector space <m>V</m> and that <m>\vvec</m> is a
        vector in <m>V</m>.  There is a unique monic polynomial
        <m>p_\vvec</m> such that
        <ul>
          <li>
            <p><m>p_\vvec(T)\vvec = 0</m>.</p>
          </li>
          <li>
            <p><m>p_\vvec</m> has the smallest possible possible
            degree among polynomials <m>p</m> for which
            <m>p(T)\vvec=0</m>.
            </p>
          </li>
          <li>
            <p><m>\deg(p_\vvec) \leq \dim V</m>.</p>
          </li>
        </ul>
        </p>
      </statement>

      <proof>
        <p>Consider the sequence of vectors formed by repeatedly
        applying <m>T</m> to <m>\vvec</m>:
        <me>
          \vvec, T\vvec, T^2\vvec, T^3\vvec,\ldots
        </me>.
        Because <m>V</m> is finite dimensional, this set of vectors
        will eventually become linearly dependent.  Suppose that
        <m>m</m> is the smallest integer for which <m>T^m</m> is a
        linear combination of the earlier vectors in the list.  This
        means that
        <md>
          <mrow>
            T^m\vvec \amp = -a_0\vvec -a_1T\vvec - \ldots -
            a_{m-1}T^{m-1}\vvec
          </mrow>
          <mrow>
            0 \amp = (a_0I + a_1T + \ldots + a_{m-1}T^{m-1} + T^m)\vvec
          </mrow>
        </md>.
        Therefore, <m>p(x) = a_0 + a_1x + \ldots +
        a_{m-1}x^{m-1}+x^m</m> is a monic polynomial for which
        <m>p(T)\vvec= 0</m>.
        </p>

        <p>Since the vectors <m>\vvec,T\vvec,\ldots,T^{m-1}\vvec</m>
        are linearly independent, there can be no polynomial <m>s</m>
        whose degree is smaller than <m>m</m> such that
        <m>s(T)\vvec=0</m>.
        </p>

        <p>Moreover, if there is another monic polynomial <m>r</m> of
        degree <m>m</m> such that <m>r(T)\vvec = 0</m>, then
        <m>p-r</m> is a polynomial whose degree is smaller than
        <m>m</m> and that satisfies <m>(p-r)(T)\vvec = 0</m>, but the
        argument of the previous paragraph shows that this cannot
        happen.  Therefore, <m>p=r</m>, which shows that <m>p</m> is
        the unique monic polynomial of smallest degree satisfying
        <m>p(T)\vvec=0</m>.
        </p>

        <p>We must have <m>\deg(p_\vvec)\leq \dim V</m> since if
        <m>\dim V = n</m>, then the set of vectors
        <m>\vvec,\ldots,T^n\vvec</m> has <m>n+1</m> vectors and hence
        must be linearly dependent.
        </p>
      </proof>
    </theorem>

    <definition>
      <statement>
        <p>The polynomial <m>p_\vvec</m> is sometimes called the
        <term>annihilating polynomial</term> of <m>\vvec</m>, since
        <m>p_\vvec(T)\vvec = 0</m>, or the <term>minimal polynomial of
        <m>T</m> applied to <m>\vvec</m>.</term>
        </p>
      </statement>
    </definition>

    <example xml:id="example-nul-range-invariant-2">
      <statement>
        <p>We will revisit <xref ref="example-nul-range-invariant"/>
        and the operator <m>T:\real^4\to\real^4</m>
        defined by <m>T\xvec=A\xvec</m> where 
        <m>A=\begin{bmatrix}
        -10 \amp 16 \amp 5 \amp -9 \\
        -6 \amp 10 \amp 1 \amp -5 \\
        -2 \amp 4 \amp 1 \amp -5 \\
        2 \amp -4 \amp -3 \amp 3 \\
        \end{bmatrix}
        </m>.
        Suppose that <m>\vvec=\fourvec1111</m> and consider the vectors
        <me>
          \vvec, T\vvec, \ldots, T^5\vvec
        </me>.
        Since this is a set
        of five vectors in <m>\real^4</m>, we are guaranteed that this
        set is linearly dependent.
        Putting these five vectors into a matrix and row reducing, we
        have
        <me>
          \begin{bmatrix}
          \vvec \amp T\vvec \amp T^2\vvec \amp T^3\vvec \amp T^4\vvec
          \end{bmatrix}
          \sim
          \begin{bmatrix}
          1 \amp 0 \amp -4 \amp 16 \amp -48 \amp 128 \\
          0 \amp 1 \amp -4 \amp 12 \amp -32 \amp 80 \\
          0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \\
          0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0
          \end{bmatrix}
        </me>.
        </p>

        <p>This shows, in particular, that <m>T^2\vvec = -4\vvec -
        4T\vvec</m> so that
        <me>
          (T^2 + 4T+4I)\vvec = 0
        </me>.
        Since <m>\vvec</m> and <m>T\vvec</m> are linearly independent,
        the minimal polynomial <m>p_\vvec</m> cannot have degree one or
        less.  Therefore,
        <me>
          p_\vvec(x) = x^2 + 4x + 4
        </me>.
        </p>
      </statement>
    </example>

    <example>
      <statement>
        <p>If <m>\vvec</m> is an eigenvector of <m>T</m> with
        associated eigenvalue <m>\lambda</m>, then
        <me>
          (T-\lambda I)\vvec = 0
        </me>
        so that <m>p_\vvec(x) = x-\lambda</m>.
        </p>
      </statement>
    </example>

    <p>The construction in this proof will be important when we
    investigate numerical linear algebra later on this course.</p>

    <definition xml:id="definition-krylov">
      <title>Krylov subspace</title>

      <statement>
        <p>Given an operator <m>T</m> on a finite dimensional vector
        space <m>V</m> and a vector <m>\vvec</m>, we define
        <me>
          \kcal_m(T,\vvec) = \laspan{\vvec,T\vvec,\ldots,T^{m-1}\vvec}
        </me>,
        the <m>m^{th}</m> <term>Krylov subspace</term>.
        </p>
      </statement>
    </definition>

    <p>We see that
    <me>
      \kcal_1\subset\kcal_2\subset\kcal_3\subset\ldots\subset\kcal_m
      = \kcal_{m+1} = \ldots = \kcal_{m+k}
    </me> so that
    the set of inclusions eventually stablizes when <m>m</m> reaches the
    degree of the minimal polynomial of <m>\vvec</m>.  This integer
    <m>m</m> is sometimes called the <term>grade</term> of
    <m>\vvec</m>.  When the grade of a vector is one, that vector must
    be an eigenvector of <m>T</m>.
    </p>
  </subsection>

  <subsection>
    <title>The minimal polynomial of an operator</title>

    <p>While we will see that the minimal polynomial of a vector
    <m>p_\vvec</m> is useful, we can take this idea a bit further and
    find a polynomial <m>p</m> that annihilates every vector in
    <m>V</m>.  This 
    will be called the <em>minimal polynomial of the operator</em>
    <m>T</m>.  Rather than starting with a proof of the theorem, let's
    illustrate the idea by continuing our earlier example from <xref
    ref="example-nul-range-invariant-2"/>.</p>

    <example xml:id="example-minimal-polynomial">
      <title>The minimal polynomial of an operator</title>

      <p>
        Remember the setup from
        <xref ref="example-nul-range-invariant-2"/>.  We have 
        the operator <m>T:\real^4\to\real^4</m>
        defined by <m>T\xvec=A\xvec</m> where 
        <m>A=\begin{bmatrix}
        -10 \amp 16 \amp 5 \amp -9 \\
        -6 \amp 10 \amp 1 \amp -5 \\
        -2 \amp 4 \amp 1 \amp -5 \\
        2 \amp -4 \amp -3 \amp 3 \\
        \end{bmatrix}
        </m>.
        With the vector <m>\vvec=\fourvec1111</m>, we found that
        <m>p_\vvec(x) = x^2 +4x+4=(x+2)^2</m>.
      </p>

      <p>We know that <m>p_\vvec(T)</m> annihilates <m>\vvec</m>,
      meaning that <m>p_\vvec(T)\vvec=0</m>.  But <m>p_\vvec(T)</m>
      also annihilates <m>T\vvec</m> since
      <me>
        p_\vvec(T)T\vvec = Tp_\vvec(T)\vvec = 0
      </me>,
      which means that <m>p_\vvec(T)</m> annihilates any vector in the
      two-dimensional Krylov subspace <m>\kcal_2(T,\vvec)</m>.  We
      would like to find a polynomial that annihilates every vector in
      <m>V</m>.</p>

      <p>To do this, will consider the subspace
      <m>U=\range(p_\vvec(T))</m>.  By
      <xref ref="prop-poly-invariant-subspaces"/>, <m>U</m> is a
      <m>T</m>-invariant subspace of <m>\real^4</m>.  In fact, we can
      find a basis for <m>U</m>.  We see that the matrix representing
      <m>p_\vvec(T)</m> is
      <me>
        p_\vvec(A) =
        \begin{bmatrix}
        -60 \amp 120 \amp 18 \amp -78 \\
        -36 \amp 72 \amp 0 \amp -36 \\
        -24 \amp 48 \amp 18 \amp -42 \\
        24 \amp -48 \amp -18 \amp 42
        \end{bmatrix}
        \sim
        \begin{bmatrix}
        1 \amp -2 \amp 0 \amp 1 \\
        0 \amp 0 \amp 1 \amp -1 \\
        0 \amp 0 \amp 0 \amp 0 \\
        0 \amp 0 \amp 0 \amp 0
        \end{bmatrix}
      </me>.
      This shows that a basis for <m>\range(p_\vvec(T)) =
      \col(p_\vvec(A))</m> is
      <me>
        \uvec_1 = \fourvec{-60}{-36}{-24}{24},\hspace{12pt}
        \uvec_2 = \fourvec{18}{0}{18}{-18}
      </me>.
      </p>

      <p>Now that we know that <m>U</m> is a <m>T</m>-invariant
      subspace and we find that
      <md>
        <mrow>
          T\uvec_1 \amp = 4\uvec_1 - 4 \uvec_2
        </mrow>
        <mrow>
          T\uvec_1 \amp = 4 \uvec_2
        </mrow>
      </md>.
      By <m>T|_U</m>, we mean the operator <m>T</m> restricted to
      <m>U</m>;  that is <m>T|_U:U\to U</m> by <m>T|_U(\uvec) =
      T\uvec</m>.  If we call the basis for <m>U</m>
      <m>\bcal=\{\uvec_1,\uvec_2\}</m>, we have
      <me>
        \coords{T|_U}{\bcal} =
        \begin{bmatrix}
        4 \amp -4 \\
        0 \amp 3 \\
        \end{bmatrix}
      </me>.
      </p>

      <p>Now we will choose a vector <m>\uvec=\twovec11</m> (think of
      this as being randomly chosen) and find the annihilating polynomial
      <m>p_\uvec</m> for <m>T|_U</m>.  We have
      <me>
        \begin{bmatrix}
        \uvec \amp T|_U\uvec \amp T|_U^2\uvec
        \end{bmatrix}
        \sim
        \begin{bmatrix}
        1 \amp 0 \amp -16 \\
        0 \amp 1 \amp 8 \\
        \end{bmatrix}
      </me>,
      which means that
      <me>
        (T|_U^2-8T|_U + 16I)\uvec = \zerovec
      </me>.
      In other words, <m>p_\uvec(x) = x^2 - 8x+16=(x-4)^2</m> is the
      annihilating polynomial of <m>\uvec</m> for the operator
      <m>T|_U</m>.
      </p>

      <p>In fact, this polynomial annihilates every vector in <m>U</m>
      because we find that
      <me>
        T|_U^2 - 8T|_U + 16I = 0
      </me>.
      </p>

      <p>Notice what happens if we multiply
      <me>
        p(x) = p_\uvec(x)p_\vvec(x) = (x-4)^2(x+2)^2
      </me>.
      Suppose that <m>\wvec</m> is any vector in <m>V</m>.  Then
      <me>
        p(T)\wvec = p_\uvec(T)p_\vvec(T)\wvec
      </me>.
      But notice that <m>p_\vvec(T)\wvec</m> is in
      <m>U=\range(p_\vvec(T))</m> and that <m>p_\uvec(T)</m>
      annihilates every vector in <m>U</m>.  This means that
      <m>p(T)\wvec = 0</m> so that <m>p(T)</m> annihilates every
      vector in <m>V</m>.
      </p>

      <p>In fact, this polynomial is what we will call the minimal
      polynomial of <m>T</m>.  This example illustrates the proof of
      the theorem that we are about to give.
      </p>
    </example>
      
    <theorem xml:id="thm-minimal-polynomial">
      <title>The minimal polynomial of an operator</title>

      <statement>
	<p>Suppose that <m>V</m> is a finite-dimensional vector space over
	either <m>\C</m> or <m>\R</m> and that <m>T</m> is an operator
	on <m>V</m>.  There is a unique monic polynomial <m>p</m> of
	smallest degree such that <m>p(T)=0</m>.  Moreover,
	<m>\deg(p)\leq \dim(V)</m>.  We call this polynomial the
	<em>minimal polynomial</em> of <m>T</m>.
	</p>
      </statement>

      <proof>
	<p>Our proof proceeds by induction on the dimension of
	<m>V</m>.  To begin, suppose that <m>\dim(V) = 1</m>, which
	means that <m>V=\laspan{\vvec}</m> for some vector
	<m>\vvec</m>.  Then <m>T\vvec = \lambda\vvec</m> for some
	<m>\lambda</m>, which is possibly <m>0</m>.  Then
	<m>(T-\lambda I)\vvec = \zerovec</m>, which means that
	<m>T-\lambda I = 0</m> since <m>\vvec</m> spans <m>V</m>.
	Therefore, if <m>p(x) = x-\lambda</m>, we have <m>p(T)=0</m>.
	</p>

	<p>Now suppose that <m>\dim(V)=n</m> and that the theorem
	has been verified for all operators on vector spaces of
	dimension less than <m>n</m>.  Following our work in
        <xref ref="example-minimal-polynomial"/>, we choose a nonzero
        vector <m>\vvec</m> and find its minimal polynomial
        <m>p_\vvec</m>. This polynomial will have degree <m>m\geq
        1</m>.
        </p>

        <p>Since <m>\nul(p_\vvec(T))</m> is <m>T</m>-invariant and
        <m>\vvec</m> is in <m>\nul(p_\vvec(T))</m>, it follows that
        <me>
          \vvec, T\vvec, T^2\vvec, \ldots, T^{m-1}\vvec
        </me>
        are all in <m>\nul(p_\vvec(T))</m>.  Since these vectors are
        linearly independent, we have
        <me>
          \dim\nul(p_\vvec(T)) \geq m \geq 1
        </me>.
        </p>

        <p>Now we focus on <m>U=\range(p_\vvec(T))</m>, which we know
        is <m>T</m>-invariant by
        <xref ref="prop-poly-invariant-subspaces"/>.  Moreover,
        by the
        <xref ref="prop-nul-range-dims" text="custom">
          Fundamental Theorem of Linear Maps
          </xref>, we know that <m>U</m> 
	<me>
	  \dim(\range(p(T))) = \dim(V) - \dim(\nul(p(T))) \leq n - m
          \lt n
	</me>.
        This says that the induction hypothesis applies to the
        operator <m>T|_U</m> so that there is a minimal polynomial,
        which we will denote by <m>p_U</m> so that <m>p_U(T|_U) =
        0</m>.  Moreover, by the induction hypothesis, this polynomial
        is monic and <m>\deg(p_U) \leq \dim U \leq n - m</m>.
	</p>

        <p>We call <m>p = p_Up_\vvec</m> and note that
        <me>
          \deg(p)=\deg(p_U)+\deg(p_\vvec) \leq n-m + m = n=\dim
          V
        </me>.
        Moreover, if <m>\wvec</m> is any vector in <m>V</m>, it
        follows that <m>p_\vvec(T)\wvec</m> is a vector in
        <m>U=\range(p_\vvec)</m> so that
        <me>
          p(T)\wvec = p_U(T)p_\vvec(T)\wvec = 0
        </me>.
        That is, <m>p(T)</m> annihilates every vector in <m>V</m> so
        that <m>p(T)=0</m>.</p>

	<p>This shows that there is a
	monic polynomial <m>p</m> such that <m>p(T)=0</m> on
	<m>V</m> and <m>\deg(p) \leq \dim V</m>.
        Therefore, there is some possibly different
	polynomial having the smallest degree among all such
	polynomials, and this is the minimal
	polynomial of the operator <m>T</m> on <m>V</m>.
	</p>

	<p>To see that this polynomial is unique, suppose there are
	two monic polynomials <m>s_1</m> and <m>s_2</m> having
	smallest degree and <m>s_1(T)=0</m> and <m>s_2(T)=0</m>.
	If we consider <m>s_1-s_2</m>, we see that <m>\deg(s_1-s_2)\lt
	\deg(s_1)=\deg(s_2)</m> since the highest degree terms of
	<m>s_1</m> and <m>s_2</m> have coefficients <m>1</m> and
	therefore cancel.  Also,
	<me>
	  (s_1-s_2)(T) = s_1(T) - s_2(T) = 0
	</me>.
	However, this is impossible since <m>s_1</m> and <m>s_2</m>
	had the smallest possible degree among all polynomials that
	vanish when evaluated on <m>T</m>.  This means that
	<m>s_1=s_2</m>, which guarantees the uniqueness of the minimal
	polynomial. 
	</p>
      </proof>
    </theorem>

    <example>
      <statement>
        <p>Returning to
        <xref ref="example-minimal-polynomial"
              text="custom">
          our earlier example
        </xref>
        where 
        <m>T:\real^4\to\real^4</m> by
        <m>T\xvec = A\xvec</m> with
        <m>A=\begin{bmatrix}
        -10 \amp 16 \amp 5 \amp -9 \\
        -6 \amp 10 \amp 1 \amp -5 \\
        -2 \amp 4 \amp 1 \amp -5 \\
        2 \amp -4 \amp -3 \amp 3 \\
        \end{bmatrix}
        </m>.
        If we choose <m>\vvec=\fourvec1010</m>.  We have
        <me>
          \begin{bmatrix}
          \vvec \amp T\vvec \amp T^2\vvec \amp T^3\vvec \amp T^4\vvec
          \end{bmatrix}
          \sim
          \begin{bmatrix}
          1 \amp 0 \amp 0 \amp 0 \amp -64 \\
          0 \amp 1 \amp 0 \amp 0 \amp -32 \\
          0 \amp 0 \amp 1 \amp 0 \amp 12 \\
          0 \amp 0 \amp 0 \amp 1 \amp 4
          \end{bmatrix}
        </me>.
        This says that <m>p_\vvec(x) = x^4-4x^3-12x^2+32x+64</m>.
        One can now check that <m>p_\vvec(T) = 0</m> so that the
        minimal polynomial of <m>T</m> is in fact
        <me>
          p(x) = x^4-4x^3-12x^2+32x+64=(x+2)^2(x-4)^2
        </me>.
        This is, of course, the same polynomial we found earlier.
        For most vectors <m>\vvec</m>, we will find that the minimal
        polynomial of the operator <m>T</m> is <m>p_\vvec</m>.
        </p>
      </statement>
    </example>
        
    <example xml:id="example-min-poly-1">
      <statement>
	<p> Consider the <m>2\times2</m> matrix 
	<m>A=\begin{bmatrix}
	2 \amp 0 \\
	0 \amp 2
	\end{bmatrix}</m> and the linear operator <m>T</m> that it
	defines.  Notice that <m>A-2I = 0</m> so if <m>p(x)=x-2</m>,
	then <m>p(T)=0</m>.  The minimal polynomial of <m>T</m> is
	therefore <m>p(x)=x-2</m>.
	</p>

	<p> More generally, suppose that the minimal polynomial of an
	operator <m>T</m> has degree <m>1</m>.  Since the minimal
	polynomial is monic, this means that <m>p(x)=x-\lambda</m>.
	Because <m>p(T)=T-\lambda I = 0</m>, we see that <m>T=\lambda
	I</m>, a multiple of the identity.
	</p>
      </statement>
    </example>

    <example xml:id="example-min-poly-2">
      <statement>
	<p> By contrast, consider the <m>2\times2</m> matrix 
	<m>B=\begin{bmatrix}
	2 &amp; 1 \\
	0 &amp; 2
	\end{bmatrix}</m> and the linear operator <m>S</m> that it
	defines with respect to some basis.  The degree of the minimal
	polynomial must be at least 2 since <m>B</m> is not a multiple
	of the identity matrix.  Notice, however, that 
	<m>B-2I = \begin{bmatrix}
	0 &amp; 1 \\
	0 &amp; 0 \\
	\end{bmatrix}
	</m> and <m>(B-2I)^2 = 0</m>.  This says that <m>(S-2I)^2 =
	0</m> and so the minimal polynomial of <m>S</m> is
	<m>q(x)=(x-2)^2</m>.
	</p>
      </statement>
    </example>

    <p> Both of the matrices in the two previous examples are
    upper triangular.  Remembering that the eigenvalues of an upper
    triangular matrix are the entries on the diagonal, we see that the
    roots of the minimal polynomials in both cases are the eigenvalues
    of the operator.  This gives some indication of the importance of
    the minimal polynomial, as we will see now.
    </p>

    <p>The fact that the minimal polynomial has the smallest degree
    among all polynomials for which <m>p(T)=0</m> has some important
    consequences. </p>

    <proposition xml:id="prop-min-poly-roots">
      <statement>
	<p>The set of roots of the minimal polynomial of <m>T</m>
	equals the set of eigenvalues of <m>T</m>.
	</p>
      </statement>

      <proof>
	<p>Suppose that <m>p</m> is the minimal polynomial of
	<m>T</m>.  We need to explain two things: that any eigenvalue
	of <m>T</m> is a root of <m>p</m> and that any root of
	<m>p</m> is an eigenvalue of <m>T</m>.
	</p>

	<p>Suppose that <m>\lambda</m> is an eigenvalue of <m>T</m>.
	This means that there is a nonzero vector <m>\vvec</m> such
	that <m>T\vvec = \lambda\vvec</m> and therefore <m>T^j\vvec =
	\lambda^j\vvec</m> for every <m>j</m>.  This means that
	<me>
	  0 = p(T)\vvec = p(\lambda)\vvec
	</me>,
	which implies that <m>p(\lambda) = 0</m>.  Therefore,
	<m>\lambda</m> is a root of <m>p</m>, the minimal polynomial
	of <m>T</m>.
	</p>

	<p>Conversely, suppose that <m>\lambda</m> is a root of
	<m>p</m>.  By <xref ref="prop-root-factor"/>, this means that
	<me>
	  p(x) = (x-\lambda)q(x)
	</me>.
	This says that
	<me>
	  0 = p(T) = (T-\lambda I)q(T)
	</me>
	However, <m>q(T)\neq 0</m> since <m>\deg(q) \lt \deg(p)</m>,
        which implies
	there is some vector <m>\vvec</m> for which <m>q(T)\vvec\neq
	0</m>.  Therefore,
	<me>
	  0 = p(T)\vvec = (T-\lambda I)q(T)\vvec
	</me>,
	which shows that <m>q(T)\vvec</m> is an eigenvector <m>T</m>
	with associated eigenvalue <m>\lambda</m>.
	</p>
      </proof>
    </proposition>

    <p>This is the most significant fact about the minimal polynomial:
    that its roots are the eigenvalues of the operator.  We'll put
    this to use in the next section.  Before that, however, here are
    two other useful facts.
    </p>
    
    <proposition xml:id="prop-min-poly-div">
      <statement>
	<p>If <m>s</m> is a polynomial for which <m>s(T)=0</m>, then
	<m>s</m> is a multiple of the minimal polynomial of <m>T</m>.
	</p>
      </statement>
      <proof>
	<p>If <m>p</m> is the minimal polynomial of <m>T</m>, then 
	we can apply the Division Algorithm to write <m>s = pq + r</m>
	where <m>\deg(r) \lt \deg(p)</m>.  Furthermore,
	<me>
	  0 = s(T) = p(T)q(T) + r(T) = r(T)
	</me>,
	which implies that <m>r(T)=0</m>.  Since <m>p</m> has the
	smallest degree among all polynomials that vanish when
	evaluated on <m>T</m> and <m>r</m> has a smaller degree than
	<m>p</m>, we know that <m>r=0</m>.  Therefore, <m>s=pq</m>.
	</p>
      </proof>
    </proposition>

    <proposition xml:id="prop-min-poly-subspace">
      <statement>
	<p>If <m>T</m> is an operator on the vector space <m>V</m> and
	<m>W</m> is a subspace of <m>V</m> that is invariant under
	<m>T</m>, then the minimal polynomial of <m>T</m> is a
	multiple of the minimal polynomial of <m>T|_W</m>.
	</p>
      </statement>
      <proof>
	<p>Suppose that <m>s</m> is the minimal polynomial of
	<m>T|_W</m> and <m>p</m> is the minimal polynomial of <m>T</m>
	on <m>V</m>.  This means that <m>p(T)\wvec=0</m> 
	for every vector <m>\wvec</m> in <m>W</m> and so <m>p(T|_W) =
	0</m>.  We also know that <m>s</m> is the minimal polynomial
	of <m>T|_W</m> so by <xref ref="prop-min-poly-div"/>, we know
	that <m>p</m> is a multiple of <m>s</m>.
	</p>
      </proof>
    </proposition>

  </subsection>
</section>
