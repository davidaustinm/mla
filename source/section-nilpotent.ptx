<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-nilpotent" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Nilpotent operators</title>

  <introduction>
    <p>The <xref ref="sec-spectral" text="custom">
    previous section</xref> gave some conditions for a matrix to be
    diagonalizable.  We will now explore what happens when a
    matrix is not diagonalizable.</p>

    <p>If an operator <m>T</m> on a vector space <m>V</m> is
    diagonalizable, then the basis <m>\bcal</m> for which
    <m>\coords{T}{\bcal}</m> is diagonal consists of eigenvectors of
    <m>T</m>.  Remember that eigenvectors of an operator <m>T</m> are
    found through the equation <m>(T-\lambda I)\vvec=\zerovec</m>.
    </p>

    <p>If we want to explore what happens when <m>T</m> is not
    diagonalizable, we will need a more general notion of
    eigenvectors.  To that end, we refer to vectors satisfying
    <me>
      (T-\lambda I)^k\vvec=\zerovec
    </me>
    for some power <m>k</m> as generalized eigenvectors.  To get
    started, however, we will first
    consider a related class of operators.
    </p>
  </introduction>

  <subsection xml:id="subsec-nul-powers">
    <title> Null spaces of powers </title>

    <p>Suppose that <m>T</m> is an operator on <m>V</m>. If
    <m>\vvec</m> is a vector for which
    <m>T^k\vvec=\zerovec</m>, then it also happens that
    <me>
      T^{k+1}\vvec=T(T^k\vvec)=\zerovec
    </me>.
    This means that  <m>\nul(T^k)\subset\nul(T^{k+1})</m>, and we
    therefore have 
    <me>
      \{0\}\subset\nul(T)\subset\nul(T^2)\subset\nul(T^3)\subset\ldots
    </me>.
    </p>

    <p>
      The next propositions say that this process stabilizes so that the
      inclusions eventually become equalities.  First we show that
      when we reach an equality, then all the following inclusions are
      equalities as well.
    </p>

    <proposition xml:id="prop-nil-nul-incl">
      <statement>
	If <m>\nul(T^m)=\nul(T^{m+1})</m>, then <m>\nul(T^{m+k}) =
	\nul(T^m)</m> for any <m>k\gt 0</m>.
      </statement>

      <proof>
	<p>Suppose that <m>\nul(T^n) = \nul(T^{n+1})</m> for some
        <m>n</m> and that <m>\vvec</m> is a vector in
        <m>\nul(T^{n+2})</m>.  It follows that
        <me>
          T^{n+2}\vvec = T^{n+1}(T\vvec) = 0
        </me>,
        which means that <m>T\vvec</m> is in <m>\nul(T^{n+1})</m>.
        Because <m>\nul(T^{n+1}) = \nul(T^n)</m>, it follows that
        <m>T\vvec</m> is in <m>\nul(T^n)</m>, which says that
        <me>
          T^{n}(T\vvec) = T^{n+1}\vvec = 0
        </me>.
        </p>

        <p>We then see that
        <me>\nul(T^m)=\nul(T^{m+1})=\nul(T^{m+2}) = \nul(T^{m+3}) =
        \ldots
        </me>.
        </p>

      </proof>
    </proposition>

    <p> The next result says that this process will always stablize by
    the time we reach the dimension of <m>V</m>.
    </p>

    <proposition xml:id="prop-null-dim-V">
      <statement>
	<p>For any operator <m>T</m> on a vector space <m>V</m> of
	dimension <m>n</m>,
	<me>
	  \nul(T^n)=\nul(T^{n+1})
	</me>.
	</p>
      </statement>

      <proof>
	<p>If <m>\nul(T)=\{\zerovec\}</m>, then <m>T</m> is invertible
	as is every power of <m>T</m>.  Therefore
	<m>\nul(T^k)=\{\zerovec\}</m> for every power, including
	<m>n</m> and <m>n+1</m>.
	</p>

	<p>Now suppose that <m>\nul(T)</m> has positive dimension so
	that <m>\dim\nul(T) \geq 1</m> and that the null spaces
	continue to grow
	<me>
	  \nul(T)\subsetneq\nul(T^2)\ldots\subsetneq\nul(T^m)
	</me>.
	In this case,
	<me>
	  n=\dim(V)\geq\dim\nul(T^m)\geq m
	</me>.
	This shows that the null spaces cannot grow beyond <m>T^n</m>
	so we have <m>\nul(T^n)=\nul(T^{n+1})</m>.
	</p>
      </proof>
    </proposition>
  </subsection>
  
  <subsection>
    <title>Nilpotent operators</title>

    <p> We will now focus on a particular type of operator known as
    nilpotent. 
    </p>

    <definition>
      <title>Nilpotent operator</title>
      <p>An operator <m>T</m> is <term>nilpotent</term> if some power
      <m>T^m=0</m>.</p>
    </definition>

    <p>Notice that the operator <m>T=0</m> is nilpotent, but there are
    certainly nonzero nilpotent operators.
    </p>
      
    <example>
      <p> Consider the matrix
      <m>A = \begin{bmatrix}
      0 \amp 1 \\
      0 \amp 0 \\
      \end{bmatrix}
      </m>
      and notice that <m>A^2=0</m>.  An operator <m>T</m> whose
      associated matrix is <m>A</m> with respect to some basis is
      nilpotent since <m>T^2=0</m>.
      </p>
    </example>

    <p>Suppose <m>T</m> is nilpotent and that <m>m</m> is the smallest
    power for which <m>T^m=0</m>.  This means
    that <m>p(T)=0</m> if <m>p(x)=x^m</m> and hence <m>p</m> is the
    minimal polynomial of <m>T</m>.  This says that <m>m\leq \dim
    V</m> so the smallest power of <m>T</m> that is zero is not more
    than the dimension of <m>V</m>.</p>

    <p>
    We could view
    <me>
      p(x) = x^m = (x-0)^m
    </me>,
    which says that there is a basis for which the matrix associated
    to <m>T</m> is upper triangular
    <me>
      \begin{bmatrix}
      0 \amp * \amp * \amp \ldots \amp * \\
      0 \amp 0 \amp * \amp \ldots \amp * \\
      \vdots \amp \vdots \amp \vdots \amp \ddots \amp * \\
      0 \amp 0 \amp 0 \amp 0 \amp 0 \\
      \end{bmatrix}
    </me>.
    </p>

    <p>In fact, we will see that there is a basis so that the matrix
    associated to a nilpotent operator has an especially nice form.
    </p>

    <definition>
      <statement>
	<p>A <term>nilpotent block</term> matrix is a square matrix having
        the form 
	<me>
	  \begin{bmatrix}
	  0 \amp 1 \amp 0 \amp \ldots \amp 0 \\
	  0 \amp 0 \amp 1 \amp \ldots \amp 0 \\
	  \vdots \amp \vdots \amp \vdots \amp \ddots \amp 1 \\
	  0 \amp 0 \amp 0 \amp 0 \amp 0 \\
	  \end{bmatrix}
	</me>.
	That is, all the entries are zero except for the entries
	directly above the diagonal, which are 1.
	</p>

      </statement>
    </definition>

    <example xml:id="example-nilpotent">
      <p>The following matrix consists of three nilpotent blocks on
      the diagonal, a <m>3\times3</m> block, a <m>2\times2</m> block,
      and a <m>1\times1</m> block.
      <me>
	\begin{bmatrix}
	0 \amp 1 \amp 0 \amp \gz \amp \gz \amp \gz \\
	0 \amp 0 \amp 1 \amp \gz \amp \gz \amp \gz \\
	0 \amp 0 \amp 0 \amp \gz \amp \gz \amp \gz \\
	\gz \amp \gz \amp \gz \amp 0 \amp 1 \amp \gz \\
	\gz \amp \gz \amp \gz \amp 0 \amp 0 \amp \gz \\
	\gz \amp \gz \amp \gz \amp \gz \amp \gz \amp 0 \\
	\end{bmatrix}
	</me>.
      </p>
        <p>Let's look a little more closely at how the linear
        transformation acts on basis vectors, which we'll denote as
        <m>\basis{\vvec}{6}</m>.  We have
        <md>
          <mrow> T\vvec_1 \amp = 0 </mrow>
          <mrow> T\vvec_2 \amp = \vvec_1 </mrow>
          <mrow> T\vvec_3 \amp = \vvec_2 </mrow>
          <mrow> T\vvec_4 \amp = 0 </mrow>
          <mrow> T\vvec_5 \amp = \vvec_4 </mrow>
          <mrow> T\vvec_6 \amp = 0 </mrow>
        </md>.
        This shows that <m>T^3=0</m>, but <m>T^2\neq
        0</m> so we have null spaces
        <me>
          \{0\}\subset\nul(T)\subset\nul(T^2)\subset\nul(T^3) = V
        </me>.
        Within these null spaces, we have bases
        <md>
          <mrow>
            \nul(T) \amp = \laspan{\vvec_1,\vvec_4, \vvec_6}
          </mrow>
          <mrow>
            \nul(T^2) \amp =\nul(T)\oplus \laspan{\vvec_2,\vvec_5}
          </mrow>
          <mrow>
            \nul(T^3) \amp =\nul(T^2)\oplus \laspan{\vvec_3}
          </mrow>          
        </md>.
        </p>
        <p>
        Notice that the <m>3\times3</m> block is formed by a vector
        <m>\vvec_3</m> that is in <m>\nul(T^3)</m> but not
        <m>\nul(T^2)</m>.  
        Once we have identified <m>\vvec_3</m>, we
        obtain new basis vectors as <m>\vvec_2=T\vvec_3</m> and
        <m>\vvec_1=T\vvec_2=T^2\vvec_3</m>.
        </p>

    </example>

    <p>In fact, every nilpotent operator has a basis whose associated
    matrix consists of a set of nilpotent blocks on the diagonal,
    which we will state and prove in the next proposition.
    </p>

    <p> First, notice that if <m>T</m> is
    nilpotent, then its minimal polynomial is <m>p(x)=x^m</m> for some
    <m>m</m>.  In particular, this means that <m>T^m=0</m> but
    <m>T^{m-1}\neq 0</m>.  As we saw in <xref
    ref="subsec-nul-powers"/>, we have the inclusion of null spaces:
    <men xml:id="eq-nil-inclusions">
      \{0\}\subsetneq\nul(T)\subsetneq\nul(T^2) \subsetneq \ldots
      \subsetneq \nul(T^m) = V
    </men>
    where the inclusion of each null space into the other is a proper
    inclusion.
    </p>

    <p>If <m>\vvec</m> is in <m>\nul(T^{j+1})</m>, then
    <me>
      0 = T^{j+1}\vvec = T^j(T\vvec)
    </me>,
    which means that <m>T\vvec</m> is in <m>\nul(T^j)</m>.  In other
    words, applying <m>T</m> pushes a vector to the left in the
    inclusions of null spaces in <xref ref="eq-nil-inclusions"/>.
    </p>

    <p> We are now ready to prove our structure theorem for nilpotent
    operators. </p>

    <proposition>
      <statement>
	<p>If <m>T</m> is a nilpotent operator on <m>V</m>, then there
	is a basis for <m>V</m> such that the matrix associated to
	<m>T</m> has the form
	<me>
	  \begin{bmatrix}
	  A_1 \amp 0 \amp \ldots \amp 0 \\
	  0 \amp A_2 \amp \ldots \amp 0 \\
	  0 \amp \vdots \amp \ddots \amp 0 \\
	  0 \amp 0 \amp 0 \amp A_k \\
	  \end{bmatrix}
	</me>
	where each <m>A_j</m> is a nilpotent block.
	</p>
      </statement>

      <proof>
        <p>
          Our proof proceeds by induction on the dimension of the
          vector space <m>V</m>, which we will denote by <m>\dim V =
          n</m>. 
        </p>

        <p>To verify the base case, suppose that <m>\dim V = n =
        1</m>.  As we have seen, if <m>\vvec</m> is a vector in
        <m>V</m>, then <m>T\vvec = \lambda\vvec</m> for some scalar
        <m>\lambda</m>.  However, if <m>T</m> is nilpotent, then
        <m>\lambda = 0</m> and so <m>T=0</m>.  In any basis, the
        matrix representing <m>T</m> is <m>[0]</m>, a <m>1\times1</m>
        nilpotent block.
        </p>

        <p>Now suppose that the result is true for any nilpotent
        operator on a vector space of dimension less than <m>n</m>.
        Suppose also that the minimal polynomial of <m>T</m> is
        <m>p(x)=x^m</m>.  This means that <m>T^m=0</m> but
        <m>T^{m-1}\neq 0</m> so
        <me>
          \nul(T^{m-1})\subsetneq\nul(T^m)
        </me>.
        We will choose a vector <m>\vvec_m</m> in <m>\nul(T^m)</m>
        that is not in <m>\nul(T^{m-1})</m> and define
        <md>
          <mrow>
            \vvec_{m-1}\amp=T\vvec_m
          </mrow>
          <mrow>
            \vvec_{m-2}\amp=T\vvec_{m-1}=T^2\vvec_m
          </mrow>
          <mrow>
            \vdots\amp=\vdots
          </mrow>
          <mrow>
            \vvec_{2}\amp=T\vvec_3 = T^{m-2}\vvec_m
          </mrow>
          <mrow>
            \vvec_{1}\amp=T\vvec_2=T^{m-1}\vvec_m
          </mrow>
        </md>.
        Notice that <m>T\vvec_1 = T^m\vvec_m=0</m> so that
        <m>\vvec_1</m> is in <m>\nul(T)</m>.  More generally,
        <m>\vvec_j</m> is in <m>\nul(T^j)</m>. 
        </p>

        <p>
        We will use <m>U</m> to denote the subspace spanned by
        <m>\basis{\vvec}{m}</m>.  Notice that a vector <m>\uvec</m> in
        <m>U</m> may be written as
        <me>
          \uvec=c_1\vvec_1 + c_2\vvec_2 + \ldots c_m\vvec_m
        </me>
        and therefore
        <me>
          T\uvec=c_2\vvec_1 + c_3\vvec_2 + \ldots c_m\vvec_{m-1}
        </me>.
        This shows that <m>U</m> is a <m>T</m>-invariant subspace of
        <m>V</m>.
        </p>
        
        <p>Using <xref ref="corollary-functional-basis"/>, suppose now
        that <m>\phi:V\to\field</m> is a linear
        functional so that
        <me>
          \phi(\vvec_1) = 1, \phi(\vvec_2) = 0,\ldots,\phi(\vvec_m) =
          0
        </me>.
        We then define <m>S:V\to \field^m</m> by
        <me>
          S(\vvec) = \left[\begin{array}{c}\phi(\vvec)\\ \phi(T\vvec) \\
          \vdots \\ \phi(T^{m-1}\vvec) \\
          \end{array}\right]
        </me>.
        Once again, if <m>\uvec</m> is a vector in <m>U</m>, then 
        <m>
          \uvec=c_1\vvec_1 + c_2\vvec_2 + \ldots c_m\vvec_m
        </m>
        so that <m>S(\uvec) = \cfourvec{c_1}{c_2}{\vdots}{c_m}</m>.
        This shows three things.
        <ul>
          <li><p> <m>S</m> is surjective.</p></li>
          <li><p> <m>S(\vvec_j)=\evec_j</m>, the standard basis vector
          in <m>\field^m</m>, which means that
          <m>\basis{\vvec}{m}</m> is a linearly independent set and
          therefore a basis for <m>U</m>.  
          Moreover, in this basis, the matrix representing
          <m>T|_U</m> is a nilpotent <m>m\times m</m> block.
          </p></li>
          <li><p> <m>S(\uvec) = 0</m> implies that <m>\uvec=0</m>,
          which means that <m>U\cap\nul(S) = \{0\}</m>.
          </p></li>
        </ul>
        </p>

        <p>Now consider <m>W=\nul(S)</m>.  Since <m>S</m> is
        surjective, we have <m>\dim W = n-m \lt n</m> by the <xref
        ref="prop-nul-range-dims">Fundamental Theorem of Linear
        Maps</xref>.  Moreover, <m>U\cap W = U\cap 
        \nul(S) = \{0\}</m> so we have <m>U\oplus W = V</m>.
        </p>

        <p>Finally, we claim that <m>W</m> is a <m>T</m>-invariant
        subspace.  Notice that <m>\wvec</m> is in <m>W=\nul(S)</m> if
        and only if <m>\phi(T^j\vvec) = 0</m> for all <m>j</m>.  If
        this is the case, then <m>\phi(T^j(T\vvec)) =
        \phi(T^{j+1}\vvec) = 0</m> for all <m>j</m>, which shows that
        <m>W</m> is <m>T</m>-invariant.
        </p>

        <p>Because <m>\dim W\lt n</m> and <m>T|_W</m> is nilpotent,
        the inductive hypothesis applies to show that there is a basis
        for <m>W</m> so that the matrix representing <m>T|_W</m>
        consists of nilpotent blocks.  We can combine this basis with
        <m>\basis{\vvec}{m}</m> to finish the proof of the theorem.
        </p>
      </proof>
    </proposition>
        <!--
        <p>We will prove this result using induction on <m>n=\dim
	V</m>.  To begin, if <m>n=\dim V = 1</m>, then <m>T=0</m> so
	the matrix associated to <m>T</m> in any basis is
	<m>\begin{bmatrix}0\end{bmatrix}</m>, a <m>1\times1</m>
	nilpotent block.
	</p>

	<p>Now suppose that the result is true for every dimension less
	than <m>n</m> and suppose that <m>m</m> is the smallest
	integer for which <m>T^m=0</m> but <m>T^{m-1}\neq 0</m>.  If
	<m>m=1</m>, then <m>T=0</m> and the matrix associated to
	<m>T</m> is the zero matrix, which is a collection of
	<m>1\times 1</m> nilpotent blocks</p>.

	<p> Now suppose that <m>m\gt 1</m> and notice
	that our assumption implies that
	<m>\nul(T^{m-1})\subsetneq\nul(T^m)</m>.  Therefore, there is 
	a vector <m>\vvec_m</m> such that <m>T^m\vvec_m=\zerovec</m> but
	<m>T^{m-1}\vvec_m \neq \zerovec</m>.
	</p>

	<p>We define <m>\vvec_j = T^{m-j}\vvec_m</m> and note that
	<m>T^j\vvec_j=\zerovec</m> but <m>T^{j-1}\vvec_j\neq
	\zerovec</m>.  Notice that the vectors <m>\vvec_j</m> form a
	linearly independent set. To see why, suppose we can find
	coefficients <m>a_j</m> such that
	<me>
	  a_1\vvec_1 + a_2\vvec_2 + \ldots + a_m\vvec_m = \zerovec.
	</me>.
	If we apply <m>T^{m-1}</m> to this vector, we have
	<me>
	  a_m\vvec_1 = \zerovec
	</me>
	which says that <m>a_m=0</m>.  Continuing in this way, shows
	that each <m>a_j=0</m>, which shows that the vectors are
	linearly independent.  We define the <m>m</m>-dimensional
	subspace <m>W</m> to be the span of these vectors.
	</p>

	<p>  Notice that <m>W</m> is invariant under <m>T</m> and
	that the matrix associated to <m>T|_W</m> in this basis is an
	<m>m\times m</m> nilpotent block.  If
	<m>W=V</m>, then the matrix associated to <m>T</m> in this
	basis is a single <m>n\times n</m> nilpotent block, and the
	conclusion of the theorem is true.
	</p>

	<p>So now suppose that <m>\dim W \lt \dim V = n</m>.  We also
	know that <m>\dim W = m \gt 1</m> so we have
	<me>
	  1 \lt \dim W \lt \dim V = n
	</me>.
	Notice that <m>W</m> is invariant under <m>T</m> and that the
	matrix associated to <m>T|_W</m> in the basis
	<m>\vvec_1,\vvec_2,\ldots,\vvec_m</m> is a <m>m\times m</m>
	nilpotent block.  To finish the proof, we need to find a
	complement to <m>W</m> inside <m>V</m> that is invariant under
	<m>T</m>. 
	</p>

	<p>To do so, we choose a linear functional <m>\phi\in V'</m>
	satisfying <m>\phi(\vvec_1)= 1</m> and <m>\phi(\vvec_j)=0</m>
	for <m>j \gt 1</m>.  We use <m>\phi</m> to define the
	linear transformation <m>\Phi:V\to\real^m</m> by
	<me>
	  \Phi(\vvec) = (\phi(\vvec),
	  \phi(T\vvec),\ldots,\phi(T^{m-1}\vvec))
	</me>.
	For a vector <m>\wvec\in W</m>, we can write
	<me>
	  \wvec = a_1\vvec_1 + a_2\vvec_2 + \ldots + a_m\vvec_m
	</me>.
	Notice that
	<me>
	  \begin{aligned}
	  \wvec \amp = a_1\vvec_1 + a_2\vvec_2 + \ldots + a_m\vvec_m \\
	  T\wvec \amp = a_2\vvec_1 + a_3\vvec_2 + \ldots + a_m\vvec_{m-1} \\
	  T^2\wvec \amp = a_3\vvec_1 + a_4\vvec_2 + \ldots +
	  a_m\vvec_{m-2} \\
	  \end{aligned}
	</me>
	so that
	<me>
	  \Phi(\wvec) = (a_1,a_2,\ldots,a_m)
	</me>.
	From this, we see that <m>\Phi</m> is surjective.
	</p>

	<p>We define the subspace
	<m>U=\nul(\Phi)</m> and note that <m>U\cap W =
	\{\zerovec\}</m> since 	<m>\Phi(\wvec) \neq \zerovec</m> if
	<m>\wvec</m> is a nonzero 
	vector in <m>W</m>.  Moreover, <m>\dim U + \dim W = \dim V</m>
	since <m>\Phi</m> is surjective.  Therefore <m>V = U\oplus
	W</m>. 
	</p>

	<p> All that remains is to show that <m>U</m> is invariant
	under <m>T</m>.  Suppose that
	<me>
	  \Phi(\vvec) =
	  (\phi(\vvec),\phi(T\vvec),\ldots,\phi(T^{m-1}\vvec)) =
	  (a_1,a_2,\ldots,a_m)
	</me>.
	Then it follows that
	<me>
	  \Phi(T\vvec) =
	  (\phi(T\vvec),\phi(T^2\vvec),\ldots,\phi(T^{m}\vvec)) =
	  (a_2,a_3,\ldots,a_m,0)
	</me>.
	Therefore, if <m>\Phi(\uvec) = \zerovec</m>, then
	<m>\Phi(T\uvec)=\zerovec</m>, which says that <m>U</m> is
	invariant under <m>T</m>.  Moreover, <m>T|_U</m> is still
	nilpotent so we know that the there is a basis for <m>U</m>
	for which the matrix of <m>T|_U</m> consists of nilpotent
	blocks.  Combining this basis with the basis of <m>W</m> we
	have constructed gives a basis of <m>V</m> for which the
	matrix associated to <m>T</m> consists of nilpotent blocks.
	</p>
      </proof>
    </proposition>
-->
  </subsection>
</section>
